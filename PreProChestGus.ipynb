{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IALAB\\.conda\\envs\\WESAD_F\\lib\\site-packages\\cupyx\\jit\\_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "import scipy.interpolate as interp\n",
    "import cupyx.scipy.signal as cusig\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import find_peaks, welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler\n",
    "faulthandler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dict = {'ACC': 700, 'ECG': 700, 'EMG': 700, 'EDA': 700, 'Temp': 700, 'Resp': 700, 'label': 700}\n",
    "WINDOW_IN_SECONDS = 30\n",
    "STRIDE_IN_SECONDS = 0.25\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 3}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 3: 'amusement'}\n",
    "feat_names = None\n",
    "DATA_PATH = r'C:\\Users\\IALAB\\Downloads\\WESAD-master\\data\\WESAD/'\n",
    "SAVE_PATH = r'C:\\Users\\IALAB\\Downloads\\WESAD-master\\data_Complete_30_025/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def eda_stats(y):\n",
    "#    Fs = fs_dict['EDA']\n",
    "#    yn = (y - y.mean()) / y.std()\n",
    "#    print(yn)\n",
    "#    print(\"calculating eda stats\")\n",
    "#    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "#    return [r, p, t, l, d, e, obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eda_metrics(eda_signal):\n",
    "    eda_signal = eda_signal.flatten()\n",
    "    scl_mean = np.mean(eda_signal)\n",
    "    scl_std = np.std(eda_signal)\n",
    "\n",
    "    diff_signal = np.diff(eda_signal)\n",
    "\n",
    "    diff_signal = diff_signal.get() if hasattr(diff_signal, \"get\") else diff_signal  # Asegurar que es un NumPy array\n",
    "    scr_peaks, _ = scisig.find_peaks(diff_signal, height=np.mean(diff_signal) + np.std(diff_signal))\n",
    "\n",
    "    scr_mean = np.mean(diff_signal[scr_peaks]) if len(scr_peaks) > 0 else 0\n",
    "    scr_std = np.std(diff_signal[scr_peaks]) if len(scr_peaks) > 0 else 0\n",
    "    scr_count = len(scr_peaks)\n",
    "    scr_amp = np.max(diff_signal[scr_peaks]) if scr_count > 0 else 0\n",
    "    scr_sum = np.sum(diff_signal[scr_peaks])\n",
    "    scr_area = np.trapezoid(diff_signal[scr_peaks])\n",
    "\n",
    "    # Correlación de SCL con el tiempo\n",
    "    time = np.arange(len(eda_signal))\n",
    "    corr_SCL_t = np.corrcoef(time, eda_signal)[0, 1] if len(eda_signal) > 1 else 0\n",
    "\n",
    "    return scl_mean, scl_std, scr_mean, scr_std, corr_SCL_t, scr_count, scr_amp, scr_sum, scr_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emg_features(emg_signal, fs=fs_dict['EMG']):\n",
    "    emg_signal = emg_signal.flatten()\n",
    "    fxx, pxx = scisig.welch(emg_signal, fs=fs, nperseg=1024)\n",
    "    \n",
    "    # Frecuencia de pico\n",
    "    f_peak = fxx[np.argmax(pxx)]\n",
    "    \n",
    "    # Bandas de frecuencia de interés\n",
    "    psd_bands = {\n",
    "        '0-10Hz': np.trapezoid(pxx[(fxx >= 0) & (fxx < 10)]),\n",
    "        '10-20Hz': np.trapezoid(pxx[(fxx >= 10) & (fxx < 20)]),\n",
    "        '20-50Hz': np.trapezoid(pxx[(fxx >= 20) & (fxx < 50)]),\n",
    "        '50-100Hz': np.trapezoid(pxx[(fxx >= 50) & (fxx < 100)]),\n",
    "        '100-150Hz': np.trapezoid(pxx[(fxx >= 100) & (fxx < 150)]),\n",
    "        '150-250Hz': np.trapezoid(pxx[(fxx >= 150) & (fxx < 250)]),\n",
    "        '250-500Hz': np.trapezoid(pxx[(fxx >= 250) & (fxx < 500)])\n",
    "    }\n",
    "    \n",
    "    psd_total = sum(psd_bands.values())\n",
    "\n",
    "    # Características de amplitud\n",
    "    peak_indices, _ = scisig.find_peaks(emg_signal, height=np.mean(emg_signal) + np.std(emg_signal))\n",
    "    peak_amp_values = emg_signal[peak_indices] if len(peak_indices) > 0 else [0]\n",
    "    peak_count = len(peak_indices)\n",
    "    peak_amp_mean = np.mean(peak_amp_values)\n",
    "    peak_amp_std = np.std(peak_amp_values)\n",
    "    peak_amp_sum = np.sum(peak_amp_values)\n",
    "    peak_amp_norm = peak_amp_sum / len(emg_signal)\n",
    "    \n",
    "    return f_peak, psd_total, peak_count, peak_amp_mean, peak_amp_std, peak_amp_sum, peak_amp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hrv_metrics(ecg_signal, fs = fs_dict['ECG']):\n",
    "    ecg_signal = ecg_signal.flatten()\n",
    "    peaks, _ = scisig.find_peaks(ecg_signal, height=np.mean(ecg_signal) + np.std(ecg_signal))\n",
    "\n",
    "    if len(peaks) < 2:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    rr_intervals = np.diff(peaks) * (1000 / fs)\n",
    "    nn50 = np.sum(np.abs(np.diff(rr_intervals)) > 50)\n",
    "    pnn50 = (nn50 / len(rr_intervals)) * 100\n",
    "    rms_hrv = np.sqrt(np.mean(np.square(np.diff(rr_intervals))))\n",
    "\n",
    "    fxx, pxx = scisig.welch(rr_intervals, fs=4.0, nperseg=len(rr_intervals))\n",
    "    lf_band = (0.04, 0.15)\n",
    "    hf_band = (0.15, 0.4)\n",
    "\n",
    "    lf = np.trapezoid(pxx[(fxx >= lf_band[0]) & (fxx <= lf_band[1])], fxx[(fxx >= lf_band[0]) & (fxx <= lf_band[1])])\n",
    "    hf = np.trapezoid(pxx[(fxx >= hf_band[0]) & (fxx <= hf_band[1])], fxx[(fxx >= hf_band[0]) & (fxx <= hf_band[1])])\n",
    "    lf_hf_ratio = lf / hf if hf > 0 else 0\n",
    "\n",
    "    sum_f = np.sum(pxx)\n",
    "    rel_f = sum_f / np.sum(rr_intervals) if np.sum(rr_intervals) > 0 else 0\n",
    "    lf_norm = (lf / (lf + hf)) * 100 if (lf + hf) > 0 else 0\n",
    "    hf_norm = (hf / (lf + hf)) * 100 if (lf + hf) > 0 else 0\n",
    "\n",
    "    hist, bin_edges = np.histogram(rr_intervals, bins=50)\n",
    "    tinn = np.max(hist) / np.mean(np.diff(bin_edges)) if np.mean(np.diff(bin_edges)) > 0 else 0\n",
    "\n",
    "    return nn50, pnn50, tinn, rms_hrv, lf, hf, lf_hf_ratio, sum_f, rel_f, lf_norm, hf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emg_peaks(emg_signal, threshold=0.05):\n",
    "    # Normalizar señal EMG\n",
    "    emg_signal = emg_signal.flatten()\n",
    "    emg_signal = (emg_signal - np.min(emg_signal)) / (np.max(emg_signal) - np.min(emg_signal))\n",
    "\n",
    "    # Detectar picos que superen el umbral\n",
    "    peaks, _ = scisig.find_peaks(emg_signal, height=threshold)\n",
    "    \n",
    "    # Obtener amplitudes de los picos detectados\n",
    "    peak_amplitudes = emg_signal[peaks] if len(peaks) > 0 else [0]\n",
    "\n",
    "    return len(peaks), np.mean(peak_amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_respiration_metrics(resp_signal, fs=fs_dict['Resp']):\n",
    "    resp_signal = resp_signal.flatten()\n",
    "    peaks, _ = scisig.find_peaks(resp_signal, height=np.mean(resp_signal) + np.std(resp_signal))\n",
    "    troughs, _ = scisig.find_peaks(-resp_signal, height=-np.mean(resp_signal) - np.std(resp_signal))\n",
    "\n",
    "    if len(peaks) < 2 or len(troughs) < 2:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    inspiration_durations = np.diff(peaks) / fs\n",
    "    expiration_durations = np.diff(troughs) / fs\n",
    "\n",
    "    I_mean = np.mean(inspiration_durations)\n",
    "    I_std = np.std(inspiration_durations)\n",
    "    E_mean = np.mean(expiration_durations)\n",
    "    E_std = np.std(expiration_durations)\n",
    "\n",
    "    ie_ratio = I_mean / E_mean if E_mean > 0 else 0\n",
    "    resp_range = np.max(resp_signal) - np.min(resp_signal)\n",
    "    insp_vol = np.mean(resp_signal[peaks]) - np.mean(resp_signal[troughs])\n",
    "    resp_rate = len(peaks) / (len(resp_signal) / fs)\n",
    "    resp_duration = len(resp_signal) / fs\n",
    "\n",
    "    return I_mean, I_std, E_mean, E_std, ie_ratio, resp_range, insp_vol, resp_rate, resp_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_chest_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.chest_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)  # Usa scipy.signal aquí\n",
    "    return cp.array(b), cp.array(a)  # Convierte los coeficientes a cupy si es necesario\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order)\n",
    "    y = cp.asnumpy(data)  # Convierte los datos de cupy a numpy antes de filtrar\n",
    "    y = scisig.lfilter(cp.asnumpy(b), cp.asnumpy(a), y)  # Aplica el filtro en numpy\n",
    "    return cp.array(y)  # Convierte el resultado de nuevo a cupy si es necesario\n",
    "\n",
    "def get_slope(signal):\n",
    "    resu = (signal[-1] - signal[0]) / len(signal)\n",
    "    resu = resu[0] if isinstance(resu, cp.ndarray) else resu\n",
    "    return resu\n",
    "\n",
    "def get_peak_freq(x, fs):\n",
    "    f, Pxx = scisig.periodogram(x, fs=fs)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    return psd_dict[max(psd_dict.keys())]\n",
    "\n",
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_net_accel(data):\n",
    "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "\n",
    "\n",
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=8)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq\n",
    "\n",
    "\n",
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data_dict, fs_dict, norm_type=None):\n",
    "    feature_dict = {}\n",
    "\n",
    "    # ECG y BVP\n",
    "    ecg_signal = data_dict['ECG']\n",
    "    feature_dict['HR_mean'] = np.mean(ecg_signal)\n",
    "    feature_dict['HR_std'] = np.std(ecg_signal)\n",
    "    \n",
    "    nn50, pNN50, tinn, rmsHRV, lf, hf, lf_hf, sum_f, rel_f, lf_norm, hf_norm = compute_hrv_metrics(ecg_signal)\n",
    "    feature_dict.update({\n",
    "        'NN50': nn50, 'pNN50': pNN50, 'TINN': tinn, \n",
    "        'rmsHRV': rmsHRV, 'LF': lf, 'HF': hf, 'LF_HF': lf_hf,\n",
    "        'sum_f': sum_f, 'rel_f': rel_f, 'LF_norm': lf_norm, 'HF_norm': hf_norm\n",
    "    })\n",
    "\n",
    "    # EDA\n",
    "    eda_signal = butter_lowpass_filter(data_dict['EDA'], 5.0, fs_dict['EDA'], 6)\n",
    "    feature_dict.update({\n",
    "        'EDA_mean': np.mean(eda_signal), 'EDA_std': np.std(eda_signal),\n",
    "        'EDA_min': np.min(eda_signal), 'EDA_max': np.max(eda_signal),\n",
    "        'EDA_range': np.max(eda_signal) - np.min(eda_signal), 'EDA_slope': get_slope(eda_signal)\n",
    "    })\n",
    "    scl_mean, scl_std, scr_mean, scr_std, corr_scl_t, scr_count, scr_amp, scr_sum, scr_area = compute_eda_metrics(eda_signal)\n",
    "\n",
    "    feature_dict.update({\n",
    "        'scl_mean': scl_mean, 'scl_std': scl_std, 'scr_mean': scr_mean, 'scr_std': scr_std,\n",
    "        'corr_scl_t': corr_scl_t, 'scr_count': scr_count, 'scr_amp': scr_amp,\n",
    "        'scr_sum': scr_sum, 'scr_area': scr_area\n",
    "    })\n",
    "\n",
    "    # EMG\n",
    "    emg_signal = data_dict['EMG']\n",
    "    feature_dict.update({\n",
    "        'EMG_mean': np.mean(emg_signal), 'EMG_std': np.std(emg_signal),\n",
    "        'EMG_median': np.median(emg_signal), 'EMG_p10': np.percentile(emg_signal, 10),\n",
    "        'EMG_p90': np.percentile(emg_signal, 90), 'EMG_range': np.max(emg_signal) - np.min(emg_signal),\n",
    "        'EMG_sum': np.sum(np.abs(emg_signal))\n",
    "    })\n",
    "    f_peak, psd_bands, peak_count, peak_amp_mean, peak_amp_std, peak_amp_sum, peak_amp_norm = compute_emg_features(emg_signal, fs_dict['EMG'])\n",
    "    feature_dict.update({\n",
    "        'EMG_f_peak': f_peak, 'EMG_PSD_bands': psd_bands,\n",
    "        'EMG_peak_count': peak_count, 'EMG_peak_amp_mean': peak_amp_mean,\n",
    "        'EMG_peak_amp_std': peak_amp_std, 'EMG_peak_amp_sum': peak_amp_sum,\n",
    "        'EMG_peak_amp_norm': peak_amp_norm\n",
    "    })\n",
    "\n",
    "    # Respiración\n",
    "    resp_signal = data_dict['Resp']\n",
    "    feature_dict.update({\n",
    "        'Resp_mean': np.mean(resp_signal), 'Resp_std': np.std(resp_signal),\n",
    "    })\n",
    "    I_mean, I_std, E_mean, E_std, ie_ratio, resp_range, insp_vol, resp_rate, resp_duration = compute_respiration_metrics(resp_signal, fs_dict['Resp'])\n",
    "    feature_dict.update({\n",
    "        'Resp_I_mean': I_mean, 'Resp_I_std': I_std, 'Resp_E_mean': E_mean, 'Resp_E_std': E_std,\n",
    "        'Resp_IE_ratio': ie_ratio, 'Resp_range': resp_range, 'Resp_insp_vol': insp_vol,\n",
    "        'Resp_rate': resp_rate, 'Resp_duration': resp_duration\n",
    "    })\n",
    "\n",
    "    # Temperatura\n",
    "    temp_signal = data_dict['Temp']\n",
    "    feature_dict.update({\n",
    "        'Temp_mean': np.mean(temp_signal), 'Temp_std': np.std(temp_signal),\n",
    "        'Temp_min': np.min(temp_signal), 'Temp_max': np.max(temp_signal),\n",
    "        'Temp_range': np.max(temp_signal) - np.min(temp_signal), 'Temp_slope': get_slope(temp_signal)\n",
    "    })\n",
    "\n",
    "    # Convertir a DataFrame con solo una fila\n",
    "    df = pd.DataFrame([feature_dict])\n",
    "\n",
    "    df[\"EDA_slope\"] = df[\"EDA_slope\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    df[\"Temp_slope\"] = df[\"Temp_slope\"].apply(lambda x: ast.literal_eval(x)[0] if isinstance(x, str) else (x[0] if isinstance(x, list) else x))\n",
    "\n",
    "\n",
    "    # Normalización opcional\n",
    "    if norm_type == 'std':\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    elif norm_type == 'minmax':\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data_dict, labels, fs_dict, stride_seconds):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    all_samples = []\n",
    "    \n",
    "    # Convertir tiempo a muestras\n",
    "    window_len = int(fs_dict['ECG'] * WINDOW_IN_SECONDS)  # Se toma una señal como referencia\n",
    "    stride_len = int(fs_dict['ECG'] * stride_seconds)  \n",
    "\n",
    "    num_ventanas = (len(labels) - window_len) // stride_len + 1\n",
    "    print(f\"El número de ventanas esperadas es: {num_ventanas}\")\n",
    "\n",
    "    last_progress = -10\n",
    "    processed = 0\n",
    "\n",
    "    all_samples = pd.DataFrame()\n",
    "\n",
    "    for start in range(0, len(labels) - window_len + 1, stride_len):\n",
    "\n",
    "        processed += 1  # Incrementar contador manualmente\n",
    "        progress = processed / num_ventanas * 100\n",
    "\n",
    "        if processed % 50 == 0:\n",
    "\n",
    "            print(f\"\\rProgreso: {progress:.4f}% completado\", end=\"\", flush=True)\n",
    "        \n",
    "        last_progress = progress\n",
    "        # Extraer ventana de cada señal\n",
    "        window_data = {key: val[start:start + window_len] for key, val in data_dict.items()}\n",
    "        window_labels = labels[start:start + window_len]  # Extraer etiquetas de la ventana\n",
    "\n",
    "        # Aplicar hard labeling: etiqueta más frecuente en la ventana\n",
    "        label_counts = Counter(window_labels)\n",
    "        most_common_labels = label_counts.most_common()  # [(label1, count1), (label2, count2), ...]\n",
    "        \n",
    "        # Si hay empate, tomar la primera que aparece en la ventana original\n",
    "        max_count = most_common_labels[0][1]\n",
    "        candidate_labels = [label for label, count in most_common_labels if count == max_count]\n",
    "        chosen_label = next(label for label in window_labels if label in candidate_labels)\n",
    "\n",
    "        # Calcular características con compute_features\n",
    "\n",
    "        features_df = compute_features(window_data, fs_dict)\n",
    "\n",
    "        # Agregar la etiqueta al DataFrame de features\n",
    "        features_df['label'] = chosen_label\n",
    "\n",
    "        all_samples = pd.concat([all_samples, features_df], ignore_index=True)\n",
    "\n",
    "    if all_samples.empty:\n",
    "        print(\"Advertencia: No se generaron muestras en get_samples(), devolviendo DataFrame vacío.\")\n",
    "\n",
    "    print(\"\\n Procesamiento de ventanas completado.\")  \n",
    "    \n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id):\n",
    "    global SAVE_PATH\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path=r'C:\\Users\\IALAB\\Downloads\\WESAD-master\\data\\WESAD', subject_number=subject_id)\n",
    "\n",
    "    # Empatica E4 data - now with resp\n",
    "    data_dict = subject.get_chest_data()\n",
    "\n",
    "    print(data_dict.keys())\n",
    "    print(subject.labels)\n",
    "\n",
    "    # norm type\n",
    "    norm_type = None\n",
    "\n",
    "    # The 3 classes we are classifying\n",
    "    grouped = subject.labels\n",
    "    print(len(grouped))\n",
    "    baseline = grouped[grouped == 1]\n",
    "    print(len(baseline))\n",
    "    stress = grouped[grouped == 2]\n",
    "    print(len(stress))\n",
    "    amusement = grouped[grouped == 3]\n",
    "    print(len(amusement))\n",
    "\n",
    "    total_data = len(baseline) + len(stress) + len(amusement)\n",
    "    print(\"DATA A TRABAJAR: \" + str(total_data))\n",
    "    \n",
    "    # print(f'Available windows for {subject.name}:')\n",
    "    samples_per_window = int(fs_dict['label'] * WINDOW_IN_SECONDS)\n",
    "    stride_per_window = int(fs_dict['label'] * STRIDE_IN_SECONDS)\n",
    "\n",
    "    n_wdws = (total_data - samples_per_window) / stride_per_window + 1\n",
    "    # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "    print(f\"Procesando S{subject_id}:\")\n",
    "    print(f\"  - Windows: {n_wdws}\")\n",
    "\n",
    "\n",
    "    valid_labels = {1, 2, 3}\n",
    "    mask = np.isin(subject.labels, list(valid_labels))\n",
    "    filtered_labels = subject.labels[mask]\n",
    "    filtered_signals = {}\n",
    "    for key in data_dict.keys():\n",
    "        filtered_signals[key] = data_dict[key][mask] \n",
    "\n",
    "    print(\"Data lista para procesar: \" + str(len(filtered_labels)))\n",
    "\n",
    "    samples = get_samples(filtered_signals, filtered_labels, fs_dict=fs_dict, stride_seconds = STRIDE_IN_SECONDS)\n",
    "\n",
    "    print(\"features calculated\")\n",
    "\n",
    "    if not isinstance(samples, pd.DataFrame):\n",
    "        all_samples = pd.concat(samples, ignore_index=True)\n",
    "    else:\n",
    "        all_samples = samples.copy()\n",
    "\n",
    "    all_samples['label'] = all_samples['label'].astype(int)\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "\n",
    "    # Guarda el archivo CSV\n",
    "    all_samples.to_csv(f'{SAVE_PATH}/S{subject_id}_feats_4.csv', index=False)\n",
    "\n",
    "    # Liberar memoria\n",
    "    subject = None\n",
    "    all_samples = None\n",
    "    samples = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{SAVE_PATH}/S{s}_feats_4.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    print(df.head(10))\n",
    "    print(df.columns)\n",
    "\n",
    "    df['label'] = df[['1', '2', '3']].idxmax(axis=1).astype(int)\n",
    "    df.drop(['1', '2', '3'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{SAVE_PATH}/may14_feats4.csv', engine='python')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "\n",
    "    print(\"Índices en counts:\", counts.index.tolist())\n",
    "    print(\"Claves en int_to_label:\", int_to_label.keys())\n",
    "\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "subject_ids = []\n",
    "\n",
    "for patient in subject_ids:\n",
    "    print(f'Processing data for S{patient}...')\n",
    "    make_patient_data(patient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             HR_std  NN50      pNN50      TINN      rmsHRV           LF  \\\n",
      "HR_mean                                                                   \n",
      " 0.001771  0.131424   134  63.207547  7.440476  346.208974  1487.321479   \n",
      " 0.001614  0.131739   136  63.255814  7.500000  343.972138  1553.809620   \n",
      "-0.000005  0.132303   136  62.100457  7.678571  338.610275  1746.214533   \n",
      " 0.001239  0.130833   136  62.385321  7.619048  333.785207  1705.577825   \n",
      " 0.001524  0.130942   138  63.013699  7.678571  334.063592  1688.604966   \n",
      " 0.000559  0.132147   137  62.557078  7.738095  336.462566  1597.203703   \n",
      " 0.000758  0.130931   138  62.443439  7.678571  324.676077  1581.800248   \n",
      " 0.001060  0.130706   138  62.727273  7.619048  325.566179  1629.204631   \n",
      " 0.001974  0.132728   137  63.720930  7.559524  342.957498  1761.606603   \n",
      "-0.000221  0.133575   137  63.720930  7.500000  328.120693  1358.006821   \n",
      "\n",
      "                    HF     LF_HF         sum_f      rel_f  ...  Temp_mean  \\\n",
      "HR_mean                                                    ...              \n",
      " 0.001771  1828.171306  0.813557  2.680981e+06  90.586797  ...  29.177286   \n",
      " 0.001614  1766.386265  0.879654  2.737165e+06  91.719282  ...  29.178220   \n",
      "-0.000005  1608.096270  1.085889  2.723978e+06  91.588690  ...  29.179161   \n",
      " 0.001239  1634.634313  1.043400  2.701462e+06  92.249556  ...  29.180103   \n",
      " 0.001524  1647.192587  1.025141  2.711467e+06  91.665551  ...  29.181095   \n",
      " 0.000559  1716.036490  0.930752  2.721781e+06  91.642463  ...  29.182108   \n",
      " 0.000758  1731.699301  0.913438  2.684564e+06  91.784433  ...  29.183096   \n",
      " 0.001060  1740.171021  0.936232  2.700680e+06  92.335462  ...  29.184042   \n",
      " 0.001974  2184.394134  0.806451  2.917272e+06  97.679620  ...  29.184982   \n",
      "-0.000221  1842.336384  0.737111  2.652029e+06  91.548478  ...  29.185900   \n",
      "\n",
      "           Temp_std   Temp_min   Temp_max  Temp_range       Temp_slope     1  \\\n",
      "HR_mean                                                                        \n",
      " 0.001771  0.045158  29.072113  29.408905    0.336792  [7.1222216e-06]  True   \n",
      " 0.001614  0.045209  29.072113  29.408905    0.336792   [4.725865e-06]  True   \n",
      "-0.000005  0.045296  29.072113  29.408905    0.336792    [4.45266e-06]  True   \n",
      " 0.001239  0.045459  29.072113  29.408905    0.336792  [4.2477564e-06]  True   \n",
      " 0.001524  0.045739  29.072113  29.408905    0.336792   [5.411784e-06]  True   \n",
      " 0.000559  0.046022  29.072113  29.408905    0.336792   [6.027948e-06]  True   \n",
      " 0.000758  0.046262  29.072113  29.408905    0.336792   [5.001976e-06]  True   \n",
      " 0.001060  0.046479  29.072113  29.408905    0.336792  [5.6166878e-06]  True   \n",
      " 0.001974  0.046653  29.072113  29.408905    0.336792  [4.3160576e-06]  True   \n",
      "-0.000221  0.046805  29.072113  29.408905    0.336792   [4.179455e-06]  True   \n",
      "\n",
      "               2      3  subject  \n",
      "HR_mean                           \n",
      " 0.001771  False  False        2  \n",
      " 0.001614  False  False        2  \n",
      "-0.000005  False  False        2  \n",
      " 0.001239  False  False        2  \n",
      " 0.001524  False  False        2  \n",
      " 0.000559  False  False        2  \n",
      " 0.000758  False  False        2  \n",
      " 0.001060  False  False        2  \n",
      " 0.001974  False  False        2  \n",
      "-0.000221  False  False        2  \n",
      "\n",
      "[10 rows x 62 columns]\n",
      "Index(['HR_std', 'NN50', 'pNN50', 'TINN', 'rmsHRV', 'LF', 'HF', 'LF_HF',\n",
      "       'sum_f', 'rel_f', 'LF_norm', 'HF_norm', 'EDA_mean', 'EDA_std',\n",
      "       'EDA_min', 'EDA_max', 'EDA_range', 'EDA_slope', 'scl_mean', 'scl_std',\n",
      "       'scr_mean', 'scr_std', 'corr_scl_t', 'scr_count', 'scr_amp', 'scr_sum',\n",
      "       'scr_area', 'EMG_mean', 'EMG_std', 'EMG_median', 'EMG_p10', 'EMG_p90',\n",
      "       'EMG_range', 'EMG_sum', 'EMG_f_peak', 'EMG_PSD_bands', 'EMG_peak_count',\n",
      "       'EMG_peak_amp_mean', 'EMG_peak_amp_std', 'EMG_peak_amp_sum',\n",
      "       'EMG_peak_amp_norm', 'Resp_mean', 'Resp_std', 'Resp_I_mean',\n",
      "       'Resp_I_std', 'Resp_E_mean', 'Resp_E_std', 'Resp_IE_ratio',\n",
      "       'Resp_range', 'Resp_insp_vol', 'Resp_rate', 'Resp_duration',\n",
      "       'Temp_mean', 'Temp_std', 'Temp_min', 'Temp_max', 'Temp_range',\n",
      "       'Temp_slope', '1', '2', '3', 'subject'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "combine_files(subject_ids)\n",
    "print('Processing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('holi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WESAD_F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
