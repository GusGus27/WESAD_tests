{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,

   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "import scipy.interpolate as interp\n",
    "import cupyx.scipy.signal as cusig\n",
    "import neurokit2 as nk\n",

    "from scipy.signal import find_peaks, welch"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler\n",
    "faulthandler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dict = {'ACC': 700, 'ECG': 700, 'EMG': 700, 'EDA': 700, 'Temp': 700, 'Resp': 700, 'label': 700}\n",
    "WINDOW_IN_SECONDS = 30\n",
    "STRIDE_IN_SECONDS = 0.75\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 3}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 3: 'amusement'}\n",
    "feat_names = None\n",
    "DATA_PATH = r'C:\\Users\\IALAB\\Downloads\\WESAD_TEST\\data\\WESAD/'\n",
    "SAVE_PATH = r'C:\\Users\\IALAB\\Downloads\\WESAD_TEST\\data_Complete_30_075/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def eda_stats(y):\n",
    "#    Fs = fs_dict['EDA']\n",
    "#    yn = (y - y.mean()) / y.std()\n",
    "#    print(yn)\n",
    "#    print(\"calculating eda stats\")\n",
    "#    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "#    return [r, p, t, l, d, e, obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eda_metrics(eda_signal):\n",
    "    eda_signal = eda_signal.flatten()\n",
    "    scl_mean = np.mean(eda_signal)\n",
    "    scl_std = np.std(eda_signal)\n",
    "\n",
    "    diff_signal = np.diff(eda_signal)\n",
    "\n",
    "    diff_signal = diff_signal.get() if hasattr(diff_signal, \"get\") else diff_signal  # Asegurar que es un NumPy array\n",
    "    scr_peaks, _ = scisig.find_peaks(diff_signal, height=np.mean(diff_signal) + np.std(diff_signal))\n",
    "\n",
    "    scr_mean = np.mean(diff_signal[scr_peaks]) if len(scr_peaks) > 0 else 0\n",
    "    scr_std = np.std(diff_signal[scr_peaks]) if len(scr_peaks) > 0 else 0\n",
    "    scr_count = len(scr_peaks)\n",
    "    scr_amp = np.max(diff_signal[scr_peaks]) if scr_count > 0 else 0\n",
    "    scr_sum = np.sum(diff_signal[scr_peaks])\n",
    "    scr_area = np.trapz(diff_signal[scr_peaks])\n",
    "\n",
    "    # Correlación de SCL con el tiempo\n",
    "    time = np.arange(len(eda_signal))\n",
    "    corr_SCL_t = np.corrcoef(time, eda_signal)[0, 1] if len(eda_signal) > 1 else 0\n",
    "\n",
    "    return scl_mean, scl_std, scr_mean, scr_std, corr_SCL_t, scr_count, scr_amp, scr_sum, scr_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emg_features(emg_signal, fs=fs_dict['EMG']):\n",
    "    emg_signal = emg_signal.flatten()\n",
    "    fxx, pxx = scisig.welch(emg_signal, fs=fs, nperseg=1024)\n",
    "    \n",
    "    # Frecuencia de pico\n",
    "    f_peak = fxx[np.argmax(pxx)]\n",
    "    \n",
    "    # Bandas de frecuencia de interés\n",
    "    psd_bands = {\n",
    "        '0-10Hz': np.trapz(pxx[(fxx >= 0) & (fxx < 10)]),\n",
    "        '10-20Hz': np.trapz(pxx[(fxx >= 10) & (fxx < 20)]),\n",
    "        '20-50Hz': np.trapz(pxx[(fxx >= 20) & (fxx < 50)]),\n",
    "        '50-100Hz': np.trapz(pxx[(fxx >= 50) & (fxx < 100)]),\n",
    "        '100-150Hz': np.trapz(pxx[(fxx >= 100) & (fxx < 150)]),\n",
    "        '150-250Hz': np.trapz(pxx[(fxx >= 150) & (fxx < 250)]),\n",
    "        '250-500Hz': np.trapz(pxx[(fxx >= 250) & (fxx < 500)])\n",
    "    }\n",
    "    \n",
    "    psd_total = sum(psd_bands.values())\n",
    "\n",
    "    # Características de amplitud\n",
    "    peak_indices, _ = scisig.find_peaks(emg_signal, height=np.mean(emg_signal) + np.std(emg_signal))\n",
    "    peak_amp_values = emg_signal[peak_indices] if len(peak_indices) > 0 else [0]\n",
    "    peak_count = len(peak_indices)\n",
    "    peak_amp_mean = np.mean(peak_amp_values)\n",
    "    peak_amp_std = np.std(peak_amp_values)\n",
    "    peak_amp_sum = np.sum(peak_amp_values)\n",
    "    peak_amp_norm = peak_amp_sum / len(emg_signal)\n",
    "    \n",
    "    return f_peak, psd_total, peak_count, peak_amp_mean, peak_amp_std, peak_amp_sum, peak_amp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hrv_metrics(ecg_signal, fs = fs_dict['ECG']):\n",
    "    ecg_signal = ecg_signal.flatten()\n",
    "    peaks, _ = scisig.find_peaks(ecg_signal, height=np.mean(ecg_signal) + np.std(ecg_signal))\n",
    "\n",
    "    if len(peaks) < 2:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    rr_intervals = np.diff(peaks) * (1000 / fs)\n",
    "    nn50 = np.sum(np.abs(np.diff(rr_intervals)) > 50)\n",
    "    pnn50 = (nn50 / len(rr_intervals)) * 100\n",
    "    rms_hrv = np.sqrt(np.mean(np.square(np.diff(rr_intervals))))\n",
    "\n",
    "    fxx, pxx = scisig.welch(rr_intervals, fs=4.0, nperseg=len(rr_intervals))\n",
    "    lf_band = (0.04, 0.15)\n",
    "    hf_band = (0.15, 0.4)\n",
    "\n",
    "    lf = np.trapz(pxx[(fxx >= lf_band[0]) & (fxx <= lf_band[1])], fxx[(fxx >= lf_band[0]) & (fxx <= lf_band[1])])\n",
    "    hf = np.trapz(pxx[(fxx >= hf_band[0]) & (fxx <= hf_band[1])], fxx[(fxx >= hf_band[0]) & (fxx <= hf_band[1])])\n",
    "    lf_hf_ratio = lf / hf if hf > 0 else 0\n",
    "\n",
    "    sum_f = np.sum(pxx)\n",
    "    rel_f = sum_f / np.sum(rr_intervals) if np.sum(rr_intervals) > 0 else 0\n",
    "    lf_norm = (lf / (lf + hf)) * 100 if (lf + hf) > 0 else 0\n",
    "    hf_norm = (hf / (lf + hf)) * 100 if (lf + hf) > 0 else 0\n",
    "\n",
    "    hist, bin_edges = np.histogram(rr_intervals, bins=50)\n",
    "    tinn = np.max(hist) / np.mean(np.diff(bin_edges)) if np.mean(np.diff(bin_edges)) > 0 else 0\n",
    "\n",
    "    return nn50, pnn50, tinn, rms_hrv, lf, hf, lf_hf_ratio, sum_f, rel_f, lf_norm, hf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emg_peaks(emg_signal, threshold=0.05):\n",
    "    # Normalizar señal EMG\n",
    "    emg_signal = emg_signal.flatten()\n",
    "    emg_signal = (emg_signal - np.min(emg_signal)) / (np.max(emg_signal) - np.min(emg_signal))\n",
    "\n",
    "    # Detectar picos que superen el umbral\n",
    "    peaks, _ = scisig.find_peaks(emg_signal, height=threshold)\n",
    "    \n",
    "    # Obtener amplitudes de los picos detectados\n",
    "    peak_amplitudes = emg_signal[peaks] if len(peaks) > 0 else [0]\n",
    "\n",
    "    return len(peaks), np.mean(peak_amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_respiration_metrics(resp_signal, fs=fs_dict['Resp']):\n",
    "    resp_signal = resp_signal.flatten()\n",
    "    peaks, _ = scisig.find_peaks(resp_signal, height=np.mean(resp_signal) + np.std(resp_signal))\n",
    "    troughs, _ = scisig.find_peaks(-resp_signal, height=-np.mean(resp_signal) - np.std(resp_signal))\n",
    "\n",
    "    if len(peaks) < 2 or len(troughs) < 2:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    inspiration_durations = np.diff(peaks) / fs\n",
    "    expiration_durations = np.diff(troughs) / fs\n",
    "\n",
    "    I_mean = np.mean(inspiration_durations)\n",
    "    I_std = np.std(inspiration_durations)\n",
    "    E_mean = np.mean(expiration_durations)\n",
    "    E_std = np.std(expiration_durations)\n",
    "\n",
    "    ie_ratio = I_mean / E_mean if E_mean > 0 else 0\n",
    "    resp_range = np.max(resp_signal) - np.min(resp_signal)\n",
    "    insp_vol = np.mean(resp_signal[peaks]) - np.mean(resp_signal[troughs])\n",
    "    resp_rate = len(peaks) / (len(resp_signal) / fs)\n",
    "    resp_duration = len(resp_signal) / fs\n",
    "\n",
    "    return I_mean, I_std, E_mean, E_std, ie_ratio, resp_range, insp_vol, resp_rate, resp_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_chest_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.chest_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)  # Usa scipy.signal aquí\n",
    "    return cp.array(b), cp.array(a)  # Convierte los coeficientes a cupy si es necesario\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order)\n",
    "    y = cp.asnumpy(data)  # Convierte los datos de cupy a numpy antes de filtrar\n",
    "    y = scisig.lfilter(cp.asnumpy(b), cp.asnumpy(a), y)  # Aplica el filtro en numpy\n",
    "    return cp.array(y)  # Convierte el resultado de nuevo a cupy si es necesario\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs  # Frecuencia de Nyquist\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def get_slope(signal):\n",
    "    resu = (signal[-1] - signal[0]) / len(signal)\n",
    "    resu = resu[0] if isinstance(resu, cp.ndarray) else resu\n",
    "    return resu\n",
    "\n",
    "def get_peak_freq(x, fs):\n",
    "    f, Pxx = scisig.periodogram(x, fs=fs)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    return psd_dict[max(psd_dict.keys())]\n",
    "\n",
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_net_accel(data):\n",
    "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "\n",
    "\n",
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=8)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq\n",
    "\n",
    "\n",
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda.flatten())\n",
    "\n",
    "def compute_hr(ecg_signal, fs):\n",
    "    # Filtro pasa bandas para eliminar ruido\n",
    "    ecg_filtered = butter_bandpass_filter(ecg_signal, lowcut=0.5, highcut=40, fs=fs, order=4)\n",
    "\n",
    "    # Detectar picos R\n",
    "    peaks, _ = find_peaks(ecg_filtered, distance=fs*0.6, height=np.mean(ecg_filtered) + np.std(ecg_filtered))\n",
    "\n",
    "    # Calcular intervalos RR (diferencias entre picos)\n",
    "    rr_intervals = np.diff(peaks) / fs  # Convertir a segundos\n",
    "\n",
    "    # Calcular HR (latidos por minuto)\n",
    "    if len(rr_intervals) > 0:\n",
    "        hr_mean = 60 / np.mean(rr_intervals)\n",
    "        hr_std = np.std(60 / rr_intervals)  # Variabilidad de HR\n",
    "    else:\n",
    "        hr_mean, hr_std = np.nan, np.nan  # En caso de que no se detecten picos\n",
    "\n",
    "    return hr_mean, hr_std\n",
    "\n",
    "def triangle(x, a, b, c):\n",
    "    return np.maximum(0, a - np.abs(x - b) / c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data_dict, fs_dict, norm_type=None):\n",
    "    feature_dict = {}\n",
    "\n",
    "    # ECG y BVP\n",
    "    ecg_signal = data_dict['ECG']\n",
    "    hr_mean, hr_std = compute_hr(ecg_signal, fs_dict['ECG'])\n",
    "    feature_dict['HR_mean'] = hr_mean\n",
    "    feature_dict['HR_std'] = hr_std\n",
    "    \n",
    "    nn50, pNN50, tinn, rmsHRV, lf, hf, lf_hf, sum_f, rel_f, lf_norm, hf_norm = compute_hrv_metrics(ecg_signal)\n",
    "    feature_dict.update({\n",
    "        'NN50': nn50, 'pNN50': pNN50, 'TINN': tinn, \n",
    "        'rmsHRV': rmsHRV, 'LF': lf, 'HF': hf, 'LF_HF': lf_hf,\n",
    "        'sum_f': sum_f, 'rel_f': rel_f, 'LF_norm': lf_norm, 'HF_norm': hf_norm\n",
    "    })\n",
    "\n",
    "    # EDA\n",
    "    eda_signal = butter_lowpass_filter(data_dict['EDA'], 5.0, fs_dict['EDA'], 6)\n",
    "    feature_dict.update({\n",
    "        'EDA_mean': np.mean(eda_signal), 'EDA_std': np.std(eda_signal),\n",
    "        'EDA_min': np.min(eda_signal), 'EDA_max': np.max(eda_signal),\n",
    "        'EDA_range': np.max(eda_signal) - np.min(eda_signal), 'EDA_slope': get_slope(eda_signal)\n",
    "    })\n",
    "    scl_mean, scl_std, scr_mean, scr_std, corr_scl_t, scr_count, scr_amp, scr_sum, scr_area = compute_eda_metrics(eda_signal)\n",
    "\n",
    "    feature_dict.update({\n",
    "        'scl_mean': scl_mean, 'scl_std': scl_std, 'scr_mean': scr_mean, 'scr_std': scr_std,\n",
    "        'corr_scl_t': corr_scl_t, 'scr_count': scr_count, 'scr_amp': scr_amp,\n",
    "        'scr_sum': scr_sum, 'scr_area': scr_area\n",
    "    })\n",
    "\n",
    "    # EMG\n",
    "    emg_signal = data_dict['EMG']\n",
    "    feature_dict.update({\n",
    "        'EMG_mean': np.mean(emg_signal), 'EMG_std': np.std(emg_signal),\n",
    "        'EMG_median': np.median(emg_signal), 'EMG_p10': np.percentile(emg_signal, 10),\n",
    "        'EMG_p90': np.percentile(emg_signal, 90), 'EMG_range': np.max(emg_signal) - np.min(emg_signal),\n",
    "        'EMG_sum': np.sum(np.abs(emg_signal))\n",
    "    })\n",
    "    f_peak, psd_bands, peak_count, peak_amp_mean, peak_amp_std, peak_amp_sum, peak_amp_norm = compute_emg_features(emg_signal, fs_dict['EMG'])\n",
    "    feature_dict.update({\n",
    "        'EMG_f_peak': f_peak, 'EMG_PSD_bands': psd_bands,\n",
    "        'EMG_peak_count': peak_count, 'EMG_peak_amp_mean': peak_amp_mean,\n",
    "        'EMG_peak_amp_std': peak_amp_std, 'EMG_peak_amp_sum': peak_amp_sum,\n",
    "        'EMG_peak_amp_norm': peak_amp_norm\n",
    "    })\n",
    "\n",
    "    # Respiración\n",
    "    resp_signal = data_dict['Resp']\n",
    "    feature_dict.update({\n",
    "        'Resp_mean': np.mean(resp_signal), 'Resp_std': np.std(resp_signal),\n",
    "    })\n",
    "    I_mean, I_std, E_mean, E_std, ie_ratio, resp_range, insp_vol, resp_rate, resp_duration = compute_respiration_metrics(resp_signal, fs_dict['Resp'])\n",
    "    feature_dict.update({\n",
    "        'Resp_I_mean': I_mean, 'Resp_I_std': I_std, 'Resp_E_mean': E_mean, 'Resp_E_std': E_std,\n",
    "        'Resp_IE_ratio': ie_ratio, 'Resp_range': resp_range, 'Resp_insp_vol': insp_vol,\n",
    "        'Resp_rate': resp_rate, 'Resp_duration': resp_duration\n",
    "    })\n",
    "\n",
    "    # Temperatura\n",
    "    temp_signal = data_dict['Temp']\n",
    "    feature_dict.update({\n",
    "        'Temp_mean': np.mean(temp_signal), 'Temp_std': np.std(temp_signal),\n",
    "        'Temp_min': np.min(temp_signal), 'Temp_max': np.max(temp_signal),\n",
    "        'Temp_range': np.max(temp_signal) - np.min(temp_signal), 'Temp_slope': get_slope(temp_signal)\n",
    "    })\n",
    "\n",
    "    # Convertir a DataFrame con solo una fila\n",
    "    df = pd.DataFrame([feature_dict])\n",
    "\n",
    "    df[\"EDA_slope\"] = df[\"EDA_slope\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    df[\"Temp_slope\"] = df[\"Temp_slope\"].apply(lambda x: ast.literal_eval(x)[0] if isinstance(x, str) else (x[0] if isinstance(x, list) else x))\n",
    "\n",
    "\n",
    "    # Normalización opcional\n",
    "    if norm_type == 'std':\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    elif norm_type == 'minmax':\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data_dict, labels, fs_dict, stride_seconds):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    all_samples = []\n",
    "    \n",
    "    # Convertir tiempo a muestras\n",
    "    window_len = int(fs_dict['ECG'] * WINDOW_IN_SECONDS)  # Se toma una señal como referencia\n",
    "    stride_len = int(fs_dict['ECG'] * stride_seconds)  \n",
    "\n",
    "    num_ventanas = (len(labels) - window_len) // stride_len + 1\n",
    "    print(f\"El número de ventanas esperadas es: {num_ventanas}\")\n",
    "\n",
    "    last_progress = -10\n",
    "    processed = 0\n",
    "\n",
    "    all_samples = pd.DataFrame()\n",
    "\n",
    "    for start in range(0, len(labels) - window_len + 1, stride_len):\n",
    "\n",
    "        processed += 1  # Incrementar contador manualmente\n",
    "        progress = processed / num_ventanas * 100\n",
    "\n",
    "        if processed % 50 == 0:\n",
    "\n",
    "            print(f\"\\rProgreso: {progress:.4f}% completado\", end=\"\", flush=True)\n",
    "        \n",
    "        last_progress = progress\n",
    "        # Extraer ventana de cada señal\n",
    "        window_data = {key: val[start:start + window_len] for key, val in data_dict.items()}\n",
    "        window_labels = labels[start:start + window_len]  # Extraer etiquetas de la ventana\n",
    "\n",
    "        # Aplicar hard labeling: etiqueta más frecuente en la ventana\n",
    "        label_counts = Counter(window_labels)\n",
    "        most_common_labels = label_counts.most_common()  # [(label1, count1), (label2, count2), ...]\n",
    "        \n",
    "        # Si hay empate, tomar la primera que aparece en la ventana original\n",
    "        max_count = most_common_labels[0][1]\n",
    "        candidate_labels = [label for label, count in most_common_labels if count == max_count]\n",
    "        chosen_label = next(label for label in window_labels if label in candidate_labels)\n",
    "\n",
    "        # Calcular características con compute_features\n",
    "\n",
    "        features_df = compute_features(window_data, fs_dict)\n",
    "\n",
    "        # Agregar la etiqueta al DataFrame de features\n",
    "        features_df['label'] = chosen_label\n",
    "\n",
    "        all_samples = pd.concat([all_samples, features_df], ignore_index=True)\n",
    "\n",
    "    if all_samples.empty:\n",
    "        print(\"Advertencia: No se generaron muestras en get_samples(), devolviendo DataFrame vacío.\")\n",
    "\n",
    "    print(\"\\n Procesamiento de ventanas completado.\")  \n",
    "    \n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id):\n",
    "    global SAVE_PATH\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path=DATA_PATH, subject_number=subject_id)\n",
    "\n",
    "    # Empatica E4 data - now with resp\n",
    "    data_dict = subject.get_chest_data()\n",
    "\n",
    "    print(data_dict.keys())\n",
    "    print(subject.labels)\n",
    "\n",
    "    # norm type\n",
    "    norm_type = None\n",
    "\n",
    "    # The 3 classes we are classifying\n",
    "    grouped = subject.labels\n",
    "    print(len(grouped))\n",
    "    baseline = grouped[grouped == 1]\n",
    "    print(len(baseline))\n",
    "    stress = grouped[grouped == 2]\n",
    "    print(len(stress))\n",
    "    amusement = grouped[grouped == 3]\n",
    "    print(len(amusement))\n",
    "\n",
    "    total_data = len(baseline) + len(stress) + len(amusement)\n",
    "    print(\"DATA A TRABAJAR: \" + str(total_data))\n",
    "    \n",
    "    # print(f'Available windows for {subject.name}:')\n",
    "    samples_per_window = int(fs_dict['label'] * WINDOW_IN_SECONDS)\n",
    "    stride_per_window = int(fs_dict['label'] * STRIDE_IN_SECONDS)\n",
    "\n",
    "    n_wdws = (total_data - samples_per_window) / stride_per_window + 1\n",
    "    # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "    print(f\"Procesando S{subject_id}:\")\n",
    "    print(f\"  - Windows: {n_wdws}\")\n",
    "\n",
    "\n",
    "    valid_labels = {1, 2, 3}\n",
    "    mask = np.isin(subject.labels, list(valid_labels))\n",
    "    filtered_labels = subject.labels[mask]\n",
    "    filtered_signals = {}\n",
    "    for key in data_dict.keys():\n",
    "        filtered_signals[key] = data_dict[key][mask] \n",
    "\n",
    "    print(\"Data lista para procesar: \" + str(len(filtered_labels)))\n",
    "\n",
    "    samples = get_samples(filtered_signals, filtered_labels, fs_dict=fs_dict, stride_seconds = STRIDE_IN_SECONDS)\n",
    "\n",
    "    print(\"features calculated\")\n",
    "\n",
    "    if not isinstance(samples, pd.DataFrame):\n",
    "        all_samples = pd.concat(samples, ignore_index=True)\n",
    "    else:\n",
    "        all_samples = samples.copy()\n",
    "\n",
    "    all_samples['label'] = all_samples['label'].astype(int)\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "\n",
    "    # Guarda el archivo CSV\n",
    "    all_samples.to_csv(f'{SAVE_PATH}/S{subject_id}_feats_4.csv', index=False)\n",
    "\n",
    "    # Liberar memoria\n",
    "    subject = None\n",
    "    all_samples = None\n",
    "    samples = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{SAVE_PATH}/S{s}_feats_4.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    print(df.head(10))\n",
    "    print(df.columns)\n",
    "\n",
    "    df['label'] = df[['1', '2', '3']].idxmax(axis=1).astype(int)\n",
    "    df.drop(['1', '2', '3'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{SAVE_PATH}/may14_feats4.csv')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "\n",
    "    print(\"Índices en counts:\", counts.index.tolist())\n",
    "    print(\"Claves en int_to_label:\", int_to_label.keys())\n",
    "\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for S2...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "4255300\n",
      "800800\n",
      "430500\n",
      "253400\n",
      "DATA A TRABAJAR: 1484700\n",
      "Procesando S2:\n",
      "  - Windows: 2789.0\n",
      "Data lista para procesar: 1484700\n",
      "El número de ventanas esperadas es: 2789\n",
      "Progreso: 98.6016% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S3...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "4545100\n",
      "798000\n",
      "448000\n",
      "262500\n",
      "DATA A TRABAJAR: 1508500\n",
      "Procesando S3:\n",
      "  - Windows: 2834.3333333333335\n",
      "Data lista para procesar: 1508500\n",
      "El número de ventanas esperadas es: 2834\n",
      "Progreso: 98.8003% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S4...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "4496100\n",
      "810601\n",
      "444500\n",
      "260400\n",
      "DATA A TRABAJAR: 1515501\n",
      "Procesando S4:\n",
      "  - Windows: 2847.6685714285713\n",
      "Data lista para procesar: 1515501\n",
      "El número de ventanas esperadas es: 2847\n",
      "Progreso: 98.3491% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S5...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "4380600\n",
      "838600\n",
      "451500\n",
      "261800\n",
      "DATA A TRABAJAR: 1551900\n",
      "Procesando S5:\n",
      "  - Windows: 2917.0\n",
      "Data lista para procesar: 1551900\n",
      "El número de ventanas esperadas es: 2917\n",
      "Progreso: 99.4172% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S6...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "4949700\n",
      "826000\n",
      "455000\n",
      "260400\n",
      "DATA A TRABAJAR: 1541400\n",
      "Procesando S6:\n",
      "  - Windows: 2897.0\n",
      "Data lista para procesar: 1541400\n",
      "El número de ventanas esperadas es: 2897\n",
      "Progreso: 98.3776% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S7...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3666600\n",
      "830200\n",
      "448000\n",
      "260401\n",
      "DATA A TRABAJAR: 1538601\n",
      "Procesando S7:\n",
      "  - Windows: 2891.6685714285713\n",
      "Data lista para procesar: 1538601\n",
      "El número de ventanas esperadas es: 2891\n",
      "Progreso: 98.5818% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S8...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3826200\n",
      "818300\n",
      "469000\n",
      "258999\n",
      "DATA A TRABAJAR: 1546299\n",
      "Procesando S8:\n",
      "  - Windows: 2906.3314285714287\n",
      "Data lista para procesar: 1546299\n",
      "El número de ventanas esperadas es: 2906\n",
      "Progreso: 99.7935% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S9...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3656100\n",
      "826000\n",
      "451500\n",
      "260400\n",
      "DATA A TRABAJAR: 1537900\n",
      "Procesando S9:\n",
      "  - Windows: 2890.3333333333335\n",
      "Data lista para procesar: 1537900\n",
      "El número de ventanas esperadas es: 2890\n",
      "Progreso: 98.6159% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S10...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3847200\n",
      "826000\n",
      "507500\n",
      "260400\n",
      "DATA A TRABAJAR: 1593900\n",
      "Procesando S10:\n",
      "  - Windows: 2997.0\n",
      "Data lista para procesar: 1593900\n",
      "El número de ventanas esperadas es: 2997\n",
      "Progreso: 98.4318% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S11...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3663100\n",
      "826000\n",
      "476000\n",
      "257600\n",
      "DATA A TRABAJAR: 1559600\n",
      "Procesando S11:\n",
      "  - Windows: 2931.6666666666665\n",
      "Data lista para procesar: 1559600\n",
      "El número de ventanas esperadas es: 2931\n",
      "Progreso: 98.9423% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S13...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3875900\n",
      "826001\n",
      "464800\n",
      "267400\n",
      "DATA A TRABAJAR: 1558201\n",
      "Procesando S13:\n",
      "  - Windows: 2929.001904761905\n",
      "Data lista para procesar: 1558201\n",
      "El número de ventanas esperadas es: 2929\n",
      "Progreso: 99.0099% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S14...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3883600\n",
      "826000\n",
      "472500\n",
      "260401\n",
      "DATA A TRABAJAR: 1558901\n",
      "Procesando S14:\n",
      "  - Windows: 2930.3352380952383\n",
      "Data lista para procesar: 1558901\n",
      "El número de ventanas esperadas es: 2930\n",
      "Progreso: 98.9761% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S15...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3676400\n",
      "822500\n",
      "480200\n",
      "260400\n",
      "DATA A TRABAJAR: 1563100\n",
      "Procesando S15:\n",
      "  - Windows: 2938.3333333333335\n",
      "Data lista para procesar: 1563100\n",
      "El número de ventanas esperadas es: 2938\n",
      "Progreso: 98.7066% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S16...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "3941700\n",
      "826000\n",
      "471101\n",
      "257600\n",
      "DATA A TRABAJAR: 1554701\n",
      "Procesando S16:\n",
      "  - Windows: 2922.3352380952383\n",
      "Data lista para procesar: 1554701\n",
      "El número de ventanas esperadas es: 2922\n",
      "Progreso: 99.2471% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n",
      "Processing data for S17...\n",
      "dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
      "[0 0 0 ... 0 0 0]\n",
      "4144000\n",
      "826700\n",
      "506100\n",
      "260400\n",
      "DATA A TRABAJAR: 1593200\n",
      "Procesando S17:\n",
      "  - Windows: 2995.6666666666665\n",
      "Data lista para procesar: 1593200\n",
      "El número de ventanas esperadas es: 2995\n",
      "Progreso: 98.4975% completado\n",
      " Procesamiento de ventanas completado.\n",
      "features calculated\n"
     ]
    }
   ],
   "source": [
    "#subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "for patient in subject_ids:\n",
    "    print(f'Processing data for S{patient}...')\n",
    "    make_patient_data(patient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             HR_std  NN50      pNN50      TINN      rmsHRV           LF  \\\n",
      "HR_mean                                                                   \n",
      " 0.001771  0.131424   134  63.207547  7.440476  346.208974  1487.321479   \n",
      " 0.001239  0.130833   136  62.385321  7.619048  333.785207  1705.577825   \n",
      " 0.000758  0.130931   138  62.443439  7.678571  324.676077  1581.800248   \n",
      "-0.000221  0.133575   137  63.720930  7.500000  328.120693  1358.006821   \n",
      " 0.001012  0.133966   137  64.319249  7.440476  344.163462  1509.047401   \n",
      " 0.001831  0.134757   136  61.261261  7.678571  331.091918  1465.974838   \n",
      " 0.000110  0.134100   137  60.619469  7.976190  327.413931  1264.384218   \n",
      " 0.000575  0.133923   137  60.619469  7.976190  326.508666  1202.831543   \n",
      "-0.000750  0.134468   136  60.714286  7.857143  323.469950  1071.756047   \n",
      " 0.000264  0.134938   137  60.888889  7.916667  328.257878  1062.709476   \n",
      "\n",
      "                    HF     LF_HF         sum_f      rel_f  ...  Temp_mean  \\\n",
      "HR_mean                                                    ...              \n",
      " 0.001771  1828.171306  0.813557  2.680981e+06  90.586797  ...  29.177286   \n",
      " 0.001239  1634.634313  1.043400  2.701462e+06  92.249556  ...  29.180103   \n",
      " 0.000758  1731.699301  0.913438  2.684564e+06  91.784433  ...  29.183096   \n",
      "-0.000221  1842.336384  0.737111  2.652029e+06  91.548478  ...  29.185900   \n",
      " 0.001012  2364.577763  0.638189  2.833264e+06  94.894018  ...  29.188696   \n",
      " 0.001831  2174.411726  0.674194  2.798809e+06  93.605651  ...  29.191488   \n",
      " 0.000110  2218.927237  0.569818  2.680832e+06  91.353428  ...  29.194527   \n",
      " 0.000575  2193.570940  0.548344  2.625839e+06  89.492558  ...  29.197794   \n",
      "-0.000750  2176.859714  0.492340  2.518137e+06  86.094367  ...  29.200731   \n",
      " 0.000264  2038.459291  0.521330  2.432222e+06  82.764826  ...  29.203066   \n",
      "\n",
      "           Temp_std   Temp_min   Temp_max  Temp_range       Temp_slope     1  \\\n",
      "HR_mean                                                                        \n",
      " 0.001771  0.045158  29.072113  29.408905    0.336792  [7.1222216e-06]  True   \n",
      " 0.001239  0.045459  29.072113  29.408905    0.336792  [4.2477564e-06]  True   \n",
      " 0.000758  0.046262  29.072113  29.408905    0.336792   [5.001976e-06]  True   \n",
      "-0.000221  0.046805  29.072113  29.408905    0.336792   [4.179455e-06]  True   \n",
      " 0.001012  0.047380  29.072113  29.408905    0.336792  [5.6166878e-06]  True   \n",
      " 0.001831  0.048292  29.072113  29.417572    0.345459  [4.5892625e-06]  True   \n",
      " 0.000110  0.049176  29.072113  29.417572    0.345459  [8.6335685e-06]  True   \n",
      " 0.000575  0.050219  29.072113  29.426208    0.354095  [5.3449357e-06]  True   \n",
      "-0.000750  0.050811  29.072113  29.426208    0.354095   [9.592692e-06]  True   \n",
      " 0.000264  0.051011  29.072113  29.426208    0.354095  [5.6181407e-06]  True   \n",
      "\n",
      "               2      3  subject  \n",
      "HR_mean                           \n",
      " 0.001771  False  False        2  \n",
      " 0.001239  False  False        2  \n",
      " 0.000758  False  False        2  \n",
      "-0.000221  False  False        2  \n",
      " 0.001012  False  False        2  \n",
      " 0.001831  False  False        2  \n",
      " 0.000110  False  False        2  \n",
      " 0.000575  False  False        2  \n",
      "-0.000750  False  False        2  \n",
      " 0.000264  False  False        2  \n",

      "\n",
      "[10 rows x 62 columns]\n",
      "Index(['HR_std', 'NN50', 'pNN50', 'TINN', 'rmsHRV', 'LF', 'HF', 'LF_HF',\n",
      "       'sum_f', 'rel_f', 'LF_norm', 'HF_norm', 'EDA_mean', 'EDA_std',\n",
      "       'EDA_min', 'EDA_max', 'EDA_range', 'EDA_slope', 'scl_mean', 'scl_std',\n",
      "       'scr_mean', 'scr_std', 'corr_scl_t', 'scr_count', 'scr_amp', 'scr_sum',\n",
      "       'scr_area', 'EMG_mean', 'EMG_std', 'EMG_median', 'EMG_p10', 'EMG_p90',\n",
      "       'EMG_range', 'EMG_sum', 'EMG_f_peak', 'EMG_PSD_bands', 'EMG_peak_count',\n",
      "       'EMG_peak_amp_mean', 'EMG_peak_amp_std', 'EMG_peak_amp_sum',\n",
      "       'EMG_peak_amp_norm', 'Resp_mean', 'Resp_std', 'Resp_I_mean',\n",
      "       'Resp_I_std', 'Resp_E_mean', 'Resp_E_std', 'Resp_IE_ratio',\n",
      "       'Resp_range', 'Resp_insp_vol', 'Resp_rate', 'Resp_duration',\n",
      "       'Temp_mean', 'Temp_std', 'Temp_min', 'Temp_max', 'Temp_range',\n",
      "       'Temp_slope', '1', '2', '3', 'subject'],\n",
      "      dtype='object')\n",
      "Índices en counts: [1, 2, 3]\n",
      "Claves en int_to_label: dict_keys([1, 2, 3])\n",
      "Number of samples per class:\n",
      "baseline: 23191\n",
      "stress: 13129\n",
      "amusement: 7293\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "combine_files(subject_ids)\n",
    "print('Processing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holi\n"
     ]
    }
   ],
   "source": [
    "print('holi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
