{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import cupy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "from scipy.interpolate import interp1d \n",
    "from collections import Counter\n",
    "import scipy.interpolate as interp\n",
    "#import cupyx.scipy.signal as cusig\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import find_peaks, welch, butter, filtfilt, convolve\n",
    "from scipy.optimize import curve_fit\n",
    "#import cupyx.scipy.signal as cpysig\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler\n",
    "faulthandler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dict = {'PPG': 1000, 'EDA': 1000, 'RESP': 1000, 'ECG': 1000, 'label': 1000}\n",
    "WINDOW_IN_SECONDS = 30\n",
    "STRIDE_IN_SECONDS = 0.75\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'cognitive load': 3}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 3: 'cognitive load'}\n",
    "feat_names = None\n",
    "DATA_PATH = r'C:\\Users\\IALAB\\Downloads\\WESAD_TEST\\dataProcessed/'\n",
    "SAVE_PATH = r'C:\\Users\\IALAB\\Downloads\\WESAD_TEST\\features_30_075/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def eda_stats(y):\n",
    "#    Fs = fs_dict['EDA']\n",
    "#    yn = (y - y.mean()) / y.std()\n",
    "#    print(yn)\n",
    "#    print(\"calculating eda stats\")\n",
    "#    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "#    return [r, p, t, l, d, e, obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_filter(data, cutoff, fs, order=4, filter_type='low'):\n",
    "    \"\"\"\n",
    "    Aplica un filtro Butterworth a los datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - data: array de la señal a filtrar.\n",
    "    - cutoff: frecuencia de corte (o tupla en caso de bandpass).\n",
    "    - fs: frecuencia de muestreo.\n",
    "    - order: orden del filtro.\n",
    "    - filter_type: 'low', 'high' o 'band'.\n",
    "\n",
    "    Retorna:\n",
    "    - Señal filtrada.\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs  # Frecuencia de Nyquist\n",
    "\n",
    "    # Normalizar la frecuencia de corte\n",
    "    if isinstance(cutoff, (list, tuple)):  # Band-pass o Band-stop\n",
    "        normal_cutoff = [c / nyq for c in cutoff]\n",
    "    else:  # Low-pass o High-pass\n",
    "        normal_cutoff = cutoff / nyq\n",
    "\n",
    "    # Crear el filtro Butterworth\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype=filter_type, analog=False)\n",
    "\n",
    "    # Aplicar el filtro con filtfilt para evitar desfases\n",
    "    return scisig.filtfilt(b, a, np.array(data))\n",
    "\n",
    "\n",
    "def get_slope(signal):\n",
    "    if len(signal) < 2:  # Evita divisiones por 0 o valores no válidos\n",
    "        return 0\n",
    "    \n",
    "    # Convertir a pandas.Series si es necesario\n",
    "    if not isinstance(signal, pd.Series):\n",
    "        signal = pd.Series(signal)\n",
    "\n",
    "    resu = (signal.iloc[-1] - signal.iloc[0]) / (len(signal) - 1)  # Evita dividir entre len(signal)\n",
    "\n",
    "    # Convertir a escalar si es un array\n",
    "    if isinstance(resu, np.ndarray):\n",
    "        resu = resu.flatten()[0]\n",
    "    \n",
    "    return resu\n",
    "\n",
    "def get_peak_freq(x, fs):\n",
    "    f, Pxx = scisig.periodogram(x, fs=fs)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    return psd_dict[max(psd_dict.keys())]\n",
    "\n",
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_net_accel(data):\n",
    "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "\n",
    "\n",
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=8)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq\n",
    "\n",
    "\n",
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda.flatten())\n",
    "\n",
    "def triangle(x, a, b, c):\n",
    "    return np.maximum(0, a - np.abs(x - b) / c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eda_metrics(eda_signal, fs, show_plots=False):\n",
    "    eda_signal = np.array(eda_signal).flatten()\n",
    "    \n",
    "    if len(eda_signal) < 2:\n",
    "        return [np.nan] * 16  # Retorna 16 NaNs si la señal no es válida\n",
    "\n",
    "    # --------- Señal total (EDA) ----------\n",
    "    eda_mean = np.mean(eda_signal)\n",
    "    eda_std = np.std(eda_signal)\n",
    "    eda_min = np.min(eda_signal)\n",
    "    eda_max = np.max(eda_signal)\n",
    "    eda_range = eda_max - eda_min\n",
    "    eda_slope = (eda_signal[-1] - eda_signal[0]) / len(eda_signal)\n",
    "\n",
    "    # --------- Estimación de componentes ----------\n",
    "    # Estimación de componente tónica (SCL): suavizado con ventana larga (10s)\n",
    "    win_scl = int(fs * 10)\n",
    "    kernel = np.ones(win_scl) / win_scl\n",
    "    scl = convolve(eda_signal, kernel, mode='same', method='auto')\n",
    "\n",
    "    # Estimación de componente fásica (SCR): diferencia entre señal y SCL\n",
    "    scr = eda_signal - scl\n",
    "\n",
    "    # --------- Métricas SCL ----------\n",
    "    scl_mean = np.mean(scl)\n",
    "    scl_std = np.std(scl)\n",
    "    \n",
    "    time = np.arange(len(scl)) / fs\n",
    "    corr_SCL_t = np.corrcoef(time, scl)[0, 1] if len(scl) > 1 else 0\n",
    "    corr_SCL_t = np.nan_to_num(corr_SCL_t)\n",
    "\n",
    "    # --------- Detección de picos SCR ----------\n",
    "    scr_peaks, _ = find_peaks(scr, height=0.01, distance=int(1.0 * fs))  # Ajustable\n",
    "    scr_values = scr[scr_peaks] if len(scr_peaks) > 0 else np.array([np.nan])\n",
    "    \n",
    "    scr_count = len(scr_peaks)\n",
    "\n",
    "    scr_mean = np.nanmean(scr_values) if scr_count > 0 else 0\n",
    "    scr_std = np.nanstd(scr_values) if scr_count > 0 else 0\n",
    "    scr_amp = np.nanmax(scr_values) if scr_count > 0 else 0\n",
    "    scr_sum = np.nansum(scr_values) if scr_count > 0 else 0\n",
    "    scr_area = np.trapz(scr_values, dx=1/fs) if scr_count > 1 else 0\n",
    "\n",
    "    if show_plots:\n",
    "        scr_norm = (scr - np.mean(scr)) / np.std(scr)\n",
    "        start_sec = 5\n",
    "        end_sec = 25\n",
    "        zoom_start = int(start_sec * fs)\n",
    "        zoom_end = int(end_sec * fs)\n",
    "\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(14, 10))\n",
    "        fig.suptitle(\"Análisis de EDA y detección de picos SCR\", fontsize=16)\n",
    "\n",
    "        # 1. Señal completa\n",
    "        axs[0].plot(eda_signal, label='EDA Filtrada', linewidth=1)\n",
    "        axs[0].plot(scl, label='SCL (Tónica)', linewidth=1)\n",
    "        axs[0].plot(scr, label='SCR (Fásica)', linewidth=1)\n",
    "        axs[0].scatter(scr_peaks, scr[scr_peaks], c='red', label='SCR Peaks')\n",
    "        axs[0].set_title(\"Señal completa con componentes\")\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True)\n",
    "\n",
    "        # 2. SCR normalizada\n",
    "        axs[1].plot(scr_norm, color='green', label='SCR Normalizada')\n",
    "        axs[1].scatter(scr_peaks, scr_norm[scr_peaks], c='red', label='Picos detectados')\n",
    "        axs[1].axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "        axs[1].set_title(\"SCR Normalizada\")\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True)\n",
    "\n",
    "        # 3. Zoom en una región\n",
    "        zoom_peaks = [p for p in scr_peaks if zoom_start <= p < zoom_end]\n",
    "        #axs[2].plot(eda_signal[zoom_start:zoom_end], label='EDA')\n",
    "        #axs[2].plot(scl[zoom_start:zoom_end], label='SCL')\n",
    "        axs[2].plot(scr[zoom_start:zoom_end], label='SCR')\n",
    "        axs[2].scatter(\n",
    "            [p - zoom_start for p in zoom_peaks],\n",
    "            [scr[p] for p in zoom_peaks],\n",
    "            c='red', label='SCR Peaks'\n",
    "        )\n",
    "        axs[2].set_title(f\"Zoom: muestras {zoom_start} a {zoom_end}\")\n",
    "        axs[2].legend()\n",
    "        axs[2].grid(True)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    return eda_mean, eda_std, eda_min, eda_max, eda_range, eda_slope, scl_mean, scl_std, scr_mean, scr_std, corr_SCL_t, scr_count, scr_amp, scr_sum, scr_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hrv_metrics(ecg_signal, fs, show_plots=False):\n",
    "    ecg_signal = np.array(ecg_signal).flatten()\n",
    "    \n",
    "    # Detección de picos R mejorada\n",
    "    peaks, _ = scisig.find_peaks(ecg_signal, distance=fs*0.4, height=np.mean(ecg_signal) + np.std(ecg_signal))\n",
    "\n",
    "    if len(peaks) < 2:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Cálculo de intervalos RR\n",
    "    rr_intervals = np.diff(peaks) * (1000 / fs)  # Convertir a milisegundos\n",
    "    rr_intervals = rr_intervals[(rr_intervals > 400) & (rr_intervals < 1500)]  # Filtro fisiológico\n",
    "\n",
    "    if len(rr_intervals) < 2:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    hr_mean = 60000 / np.mean(rr_intervals) if len(rr_intervals) > 0 else np.nan\n",
    "    hr_std = np.std(60000 / rr_intervals) if len(rr_intervals) > 0 else np.nan\n",
    "\n",
    "    # Features temporales\n",
    "    nn50 = np.sum(np.abs(np.diff(rr_intervals)) > 50)\n",
    "    pnn50 = (nn50 / len(rr_intervals)) * 100 if len(rr_intervals) > 0 else np.nan\n",
    "    rms_hrv = np.sqrt(np.mean(np.square(np.diff(rr_intervals))))\n",
    "\n",
    "    # Cálculo de TINN\n",
    "    hist, bin_edges = np.histogram(rr_intervals, bins='auto')\n",
    "    tinn = bin_edges[np.argmax(hist)] if len(hist) > 0 else np.nan\n",
    "\n",
    "    # Interpolación mejorada\n",
    "    time_rr = np.cumsum(rr_intervals) / 1000\n",
    "    time_rr = time_rr - time_rr[0]  # Normalizar inicio en 0\n",
    "\n",
    "    fs_resample = 4.0  # Frecuencia de resampleo\n",
    "    time_resampled = np.arange(0, time_rr[-1], 1 / fs_resample)\n",
    "    interp_func = interp1d(time_rr, rr_intervals, kind=\"linear\", fill_value=\"extrapolate\")\n",
    "    rr_resampled = interp_func(time_resampled)\n",
    "\n",
    "    # Análisis espectral (PSD)\n",
    "    fxx, pxx = scisig.welch(rr_resampled, fs=fs_resample, nperseg=min(len(rr_resampled), 256))\n",
    "    lf_band, hf_band = (0.04, 0.15), (0.15, 0.4)\n",
    "\n",
    "    lf_mask = (fxx >= lf_band[0]) & (fxx <= lf_band[1])\n",
    "    hf_mask = (fxx >= hf_band[0]) & (fxx <= hf_band[1])\n",
    "\n",
    "    lf = np.trapz(pxx[lf_mask], fxx[lf_mask]) if np.any(lf_mask) else np.nan\n",
    "    hf = np.trapz(pxx[hf_mask], fxx[hf_mask]) if np.any(hf_mask) else np.nan\n",
    "    lf_hf_ratio = lf / hf if (hf > 0 and not np.isnan(hf)) else np.nan\n",
    "\n",
    "    sum_f = np.trapz(pxx, fxx)\n",
    "    rel_f = (lf + hf) / sum_f if sum_f > 0 else np.nan\n",
    "    lf_norm = (lf / (lf + hf)) * 100 if (lf + hf) > 0 else np.nan\n",
    "    hf_norm = (hf / (lf + hf)) * 100 if (lf + hf) > 0 else np.nan\n",
    "\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(ecg_signal, label=\"ECG\", color='gray')\n",
    "        plt.scatter(peaks, ecg_signal[peaks], color=\"red\", label=\"Picos R\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Detección de Picos R en ECG\")\n",
    "        plt.xlabel(\"Tiempo (ms)\")\n",
    "        plt.ylabel(\"Amplitud\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(time_rr, rr_intervals, \"o-\", label=\"Original RR\")\n",
    "        plt.plot(time_resampled, rr_resampled, \"x-\", label=\"Interpolado\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Interpolación de RR intervals\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(fxx, pxx)  # Usa escala logarítmica\n",
    "        plt.axvspan(0.04, 0.15, color=\"blue\", alpha=0.3, label=\"LF Band\")\n",
    "        plt.axvspan(0.15, 0.4, color=\"red\", alpha=0.3, label=\"HF Band\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Espectro HRV\")\n",
    "        plt.xlabel(\"Frecuencia (Hz)\")\n",
    "        plt.ylabel(\"PSD\")\n",
    "        plt.show()\n",
    "\n",
    "    return hr_mean, hr_std, nn50, pnn50, tinn, rms_hrv, lf, hf, lf_hf_ratio, sum_f, rel_f, lf_norm, hf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_respiration_metrics(resp_signal, fs, show_plots=False):\n",
    "    resp_signal = np.array(resp_signal).flatten()\n",
    "    peaks, _ = scisig.find_peaks(resp_signal, distance=fs * 0.3)\n",
    "    troughs, _ = scisig.find_peaks(-resp_signal, distance=fs * 0.3)\n",
    "    \n",
    "    if len(peaks) < 2 or len(troughs) < 2:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    inspiration_durations = np.diff(peaks) / fs\n",
    "    expiration_durations = np.diff(troughs) / fs\n",
    "\n",
    "    I_mean = np.mean(inspiration_durations)\n",
    "    I_std = np.std(inspiration_durations)\n",
    "    E_mean = np.mean(expiration_durations)\n",
    "    E_std = np.std(expiration_durations)\n",
    "\n",
    "    ie_ratio = I_mean / E_mean if E_mean > 0 else 0\n",
    "    resp_range = np.max(resp_signal) - np.min(resp_signal)\n",
    "    insp_vol = np.mean(resp_signal[peaks]) - np.mean(resp_signal[troughs])\n",
    "    resp_rate = len(peaks) / (len(resp_signal) / fs)\n",
    "    resp_duration = len(resp_signal) / fs\n",
    "\n",
    "    if show_plots:\n",
    "        plt.plot(resp_signal, label=\"Señal Respiratoria\")\n",
    "        plt.scatter(peaks, resp_signal[peaks], color='red', label=\"Picos\")\n",
    "        plt.scatter(troughs, resp_signal[troughs], color='blue', label=\"Valles\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return I_mean, I_std, E_mean, E_std, ie_ratio, resp_range, insp_vol, resp_rate, resp_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppg_features(ppg_signal, fs, show_plots=False):\n",
    "    ppg_signal = np.array(ppg_signal).flatten()\n",
    "\n",
    "    if len(ppg_signal) < 2:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Detección de picos en PPG\n",
    "    peak_indices, _ = scisig.find_peaks(ppg_signal, distance=fs*0.4, prominence=(np.std(ppg_signal) * 0.5, None))\n",
    "\n",
    "    if len(peak_indices) < 2:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    ibi = np.diff(peak_indices) / fs\n",
    "    hr = 60 / ibi\n",
    "\n",
    "    hr_mean = np.mean(hr)\n",
    "    hr_std = np.std(hr)\n",
    "    rmssd = np.sqrt(np.mean(np.diff(ibi) ** 2))\n",
    "    sdnn = np.std(ibi)\n",
    "\n",
    "    # Frecuencia de muestreo para IBI\n",
    "    fs_ibi = max(1 / np.mean(ibi), 0.5)  # No menor a 0.5 Hz\n",
    "\n",
    "    # PSD con Welch si hay suficientes puntos\n",
    "    if len(ibi) > 4:  \n",
    "        fxx, pxx = scisig.welch(ibi, fs=fs_ibi, nperseg=min(128, len(ibi)))\n",
    "    else:\n",
    "        fxx, pxx = np.array([]), np.array([])\n",
    "\n",
    "    # Cálculo de LF y HF solo si hay valores en la PSD\n",
    "    if np.sum(pxx) > 0:\n",
    "        lf_band = (fxx >= 0.04) & (fxx < 0.15)\n",
    "        hf_band = (fxx >= 0.15) & (fxx < 0.4)\n",
    "\n",
    "        lf = np.trapz(pxx[lf_band]) if np.any(lf_band) else np.nan\n",
    "        hf = np.trapz(pxx[hf_band]) if np.any(hf_band) else np.nan\n",
    "        lf_hf_ratio = lf / hf if hf > 0 else np.nan\n",
    "    else:\n",
    "        lf, hf, lf_hf_ratio = np.nan, np.nan, np.nan\n",
    "\n",
    "    # Características adicionales\n",
    "    amplitudes = ppg_signal[peak_indices]\n",
    "    pav_mean = np.mean(amplitudes)\n",
    "    pav_std = np.std(amplitudes)\n",
    "\n",
    "    rise_times = []\n",
    "    for peak in peak_indices:\n",
    "        # Buscar valle anterior\n",
    "        start = max(peak - int(0.5 * fs), 0)\n",
    "        valley_region = ppg_signal[start:peak]\n",
    "        if len(valley_region) == 0:\n",
    "            continue\n",
    "        valley_idx = np.argmin(valley_region) + start\n",
    "        rise_times.append((peak - valley_idx) / fs)\n",
    "\n",
    "    decay_times = []\n",
    "    for peak in peak_indices:\n",
    "        end = min(peak + int(0.5 * fs), len(ppg_signal))\n",
    "        decay_region = ppg_signal[peak:end]\n",
    "        if len(decay_region) == 0:\n",
    "            continue\n",
    "        valley_idx = np.argmin(decay_region) + peak\n",
    "        decay_times.append((valley_idx - peak) / fs)\n",
    "\n",
    "\n",
    "    ri_list = []\n",
    "    for peak in peak_indices:\n",
    "        end = min(peak + int(0.5 * fs), len(ppg_signal))\n",
    "        secondary_peaks, _ = scisig.find_peaks(ppg_signal[peak:end], prominence=0.1)\n",
    "        if len(secondary_peaks) > 0:\n",
    "            sec_peak_amp = ppg_signal[peak + secondary_peaks[0]]\n",
    "            ri_list.append(sec_peak_amp / ppg_signal[peak])\n",
    "        else:\n",
    "            # Alternativa: usar mínimo como valle reflejado\n",
    "            min_idx = np.argmin(ppg_signal[peak:end])\n",
    "            ri_list.append(ppg_signal[peak + min_idx] / ppg_signal[peak])\n",
    "\n",
    "    rise_time = np.mean(rise_times)\n",
    "\n",
    "    decay_time = np.mean(decay_times)\n",
    "\n",
    "    ri = np.mean(ri_list)\n",
    "\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(ppg_signal, label=\"ECG\", color='gray')\n",
    "        plt.scatter(peak_indices, ppg_signal[peak_indices], color=\"red\", label=\"Picos R\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Detección de Picos R en ECG\")\n",
    "        plt.xlabel(\"Tiempo (ms)\")\n",
    "        plt.ylabel(\"Amplitud\")\n",
    "        plt.show()\n",
    "\n",
    "    return hr_mean, hr_std, rmssd, sdnn, lf, hf, lf_hf_ratio, len(peak_indices), rise_time, decay_time, pav_mean, pav_std, ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectData:\n",
    "    def __init__(self, main_path, subject_number, part=1, total_parts=1):\n",
    "        self.name = f'{subject_number:03d}'\n",
    "        files = sorted([f for f in os.listdir(main_path) if f.startswith(self.name)])\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No se encontró archivo para el sujeto {self.name}\")\n",
    "\n",
    "        # Cargar todo el CSV\n",
    "        full_data = pd.read_csv(os.path.join(main_path, files[0]))\n",
    "\n",
    "        # Calcular índices para recorte\n",
    "        total_len = len(full_data)\n",
    "        start = (part - 1) * (total_len // total_parts)\n",
    "        end = part * (total_len // total_parts) if part < total_parts else total_len\n",
    "\n",
    "        # Guardar solo la parte seleccionada\n",
    "        self.data = full_data.iloc[start:end].reset_index(drop=True)\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data.drop(columns=['time', 'label'])\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SubjectData:\\n    def __init__(self, main_path, subject_number):\\n        self.name = f\\'{subject_number:03d}\\'  # Asegurar que el número tenga formato 000\\n        file_path = os.path.join(main_path, f\"{self.name}*.csv\")  # Ruta de los archivos CSV\\n\\n        files = sorted([f for f in os.listdir(os.path.join(main_path)) if f.startswith(self.name)])\\n        if not files:\\n            raise FileNotFoundError(f\"No se encontró archivo para el sujeto {self.name}\")\\n\\n        self.data = pd.read_csv(os.path.join(main_path, files[0]))\\n\\n        self.labels = self.data[\\'label\\']\\n\\n    def get_data(self):\\n        # Retorna los datos fisiológicos excepto el tiempo y la etiqueta\\n        return self.data.drop(columns=[\\'time\\', \\'label\\'])'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class SubjectData:\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'{subject_number:03d}'  # Asegurar que el número tenga formato 000\n",
    "        file_path = os.path.join(main_path, f\"{self.name}*.csv\")  # Ruta de los archivos CSV\n",
    "\n",
    "        files = sorted([f for f in os.listdir(os.path.join(main_path)) if f.startswith(self.name)])\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No se encontró archivo para el sujeto {self.name}\")\n",
    "\n",
    "        self.data = pd.read_csv(os.path.join(main_path, files[0]))\n",
    "\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_data(self):\n",
    "        # Retorna los datos fisiológicos excepto el tiempo y la etiqueta\n",
    "        return self.data.drop(columns=['time', 'label'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data_dict, fs_dict):\n",
    "    feature_dict = {}\n",
    "    # ECG y BVP\n",
    "\n",
    "    ecg_signal = data_dict['ECG']\n",
    "    eda_signal = data_dict['EDA']\n",
    "    ppg_signal = data_dict['PPG']\n",
    "    resp_signal = data_dict['RESP']\n",
    "\n",
    "\n",
    "    hr_mean, hr_std, nn50, pNN50, tinn, rmsHRV, lf, hf, lf_hf, sum_f, rel_f, lf_norm, hf_norm = compute_hrv_metrics(ecg_signal, fs_dict['ECG'])\n",
    "\n",
    "    feature_dict.update({\n",
    "        'HR_mean': hr_mean, 'HR_std' : hr_std,\n",
    "        'NN50': nn50, 'pNN50': pNN50, 'TINN': tinn, \n",
    "        'rmsHRV': rmsHRV, 'LF': lf, 'HF': hf, 'LF_HF': lf_hf,\n",
    "        'sum_f': sum_f, 'rel_f': rel_f, 'LF_norm': lf_norm, 'HF_norm': hf_norm\n",
    "    })\n",
    "\n",
    "\n",
    "    hr_mean, hr_std, rmssd, sdnn, lf, hf, lf_hf, num_beats, rise_time, decay_time, pav_mean, pav_std, ri = compute_ppg_features(ppg_signal, fs_dict['PPG'])\n",
    "\n",
    "    feature_dict.update({\n",
    "        'PPG_HR_mean': hr_mean, 'PPG_HR_std': hr_std, 'PPG_RMSSD': rmssd, 'PPG_SDNN': sdnn,\n",
    "        'PPG_LF': lf, 'PPG_HF': hf, 'PPG_LF_HF': lf_hf, 'PPG_num_beats': num_beats,\n",
    "        'PPG_RiseTime': rise_time, 'PPG_DecayTime': decay_time, \n",
    "        'PPG_PAV_mean': pav_mean, 'PPG_PAV_std': pav_std, 'PPG_RI': ri\n",
    "    })\n",
    "\n",
    "    eda_mean, eda_std, eda_min, eda_max, eda_range, eda_slope, scl_mean, scl_std, scr_mean, scr_std, corr_scl_t, scr_count, scr_amp, scr_sum, scr_area = compute_eda_metrics(eda_signal, fs_dict['EDA'])\n",
    "\n",
    "    feature_dict.update({\n",
    "        'EDA_mean': eda_mean, 'EDA_std': eda_std,\n",
    "        'EDA_min': eda_min, 'EDA_max': eda_max,\n",
    "        'EDA_range': eda_range, 'EDA_slope': eda_slope,\n",
    "        'scl_mean': scl_mean, 'scl_std': scl_std, 'scr_mean': scr_mean, 'scr_std': scr_std,\n",
    "        'corr_scl_t': corr_scl_t, 'scr_count': scr_count, 'scr_amp': scr_amp,\n",
    "        'scr_sum': scr_sum, 'scr_area': scr_area\n",
    "    })\n",
    "\n",
    "    feature_dict.update({\n",
    "        'Resp_mean': np.mean(resp_signal), 'Resp_std': np.std(resp_signal),\n",
    "    })\n",
    "\n",
    "    I_mean, I_std, E_mean, E_std, ie_ratio, resp_range, insp_vol, resp_rate, resp_duration = compute_respiration_metrics(resp_signal, fs_dict['RESP'])\n",
    "              \n",
    "    feature_dict.update({\n",
    "        'Resp_I_mean': I_mean, 'Resp_I_std': I_std, 'Resp_E_mean': E_mean, 'Resp_E_std': E_std,\n",
    "        'Resp_IE_ratio': ie_ratio, 'Resp_range': resp_range, 'Resp_insp_vol': insp_vol,\n",
    "        'Resp_rate': resp_rate, 'Resp_duration': resp_duration\n",
    "    })\n",
    "\n",
    "    # Convertir a DataFrame con solo una fila\n",
    "    df = pd.DataFrame([feature_dict])\n",
    "\n",
    "    df[\"EDA_slope\"] = df[\"EDA_slope\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data_dict, labels, fs_dict, stride_seconds):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    all_samples = []\n",
    "    \n",
    "    # Convertir tiempo a muestras\n",
    "    window_len = int(fs_dict['ECG'] * WINDOW_IN_SECONDS)  # Se toma una señal como referencia\n",
    "    stride_len = int(fs_dict['ECG'] * stride_seconds)  \n",
    "\n",
    "    num_ventanas = (len(labels) - window_len) // stride_len + 1\n",
    "    print(f\"El número de ventanas esperadas es: {num_ventanas}\")\n",
    "\n",
    "    last_progress = -10\n",
    "    processed = 0\n",
    "\n",
    "    all_samples = pd.DataFrame()\n",
    "\n",
    "    for start in range(0, len(labels) - window_len + 1, stride_len):\n",
    "        end = start + window_len\n",
    "\n",
    "        processed += 1  # Incrementar contador manualmente\n",
    "        progress = processed / num_ventanas * 100\n",
    "\n",
    "        if processed % 500 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        if processed % 50 == 0:\n",
    "\n",
    "            print(f\"\\rProgreso: {progress:.4f}% completado\", end=\"\", flush=True)\n",
    "\n",
    "        # Extraer ventana de cada señal\n",
    "        #print(f\"\\nVentana {start} - {end} - Numero {processed}\", end=\"\", flush= True)\n",
    "        window_data = {key: val[start:end] for key, val in data_dict.items()}\n",
    "        window_labels = labels[start:end]  # Extraer etiquetas de la ventana\n",
    "\n",
    "        # Aplicar hard labeling: etiqueta más frecuente en la ventana\n",
    "        label_counts = Counter(window_labels)\n",
    "        most_common_labels = label_counts.most_common()  # [(label1, count1), (label2, count2), ...]\n",
    "        \n",
    "        # Si hay empate, tomar la primera que aparece en la ventana original\n",
    "        max_count = most_common_labels[0][1]\n",
    "        candidate_labels = [label for label, count in most_common_labels if count == max_count]\n",
    "        chosen_label = next(label for label in window_labels if label in candidate_labels)\n",
    "\n",
    "        features_df = compute_features(window_data, fs_dict)\n",
    "\n",
    "        # Agregar la etiqueta al DataFrame de features\n",
    "        features_df['label'] = chosen_label\n",
    "\n",
    "        all_samples = pd.concat([all_samples, features_df], ignore_index=True)\n",
    "\n",
    "    if all_samples.empty:\n",
    "        print(\"Advertencia: No se generaron muestras en get_samples(), devolviendo DataFrame vacío.\")\n",
    "\n",
    "    print(\"\\n Procesamiento de ventanas completado.\")  \n",
    "    \n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id):\n",
    "    global SAVE_PATH\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Crear objeto SubjectData\n",
    "    subject = SubjectData(main_path=DATA_PATH, subject_number=subject_id)\n",
    "\n",
    "    # Obtener datos del pecho (ahora en DataFrame)\n",
    "    data_df = subject.get_data()\n",
    "\n",
    "    print(\"Columnas disponibles:\", data_df.columns)\n",
    "    print(\"Etiquetas:\", subject.labels.value_counts())\n",
    "\n",
    "    # Definir clases de interés\n",
    "    valid_labels = {1, 2, 3}\n",
    "    \n",
    "    # Filtrar datos válidos\n",
    "    mask = subject.labels.isin(valid_labels)\n",
    "    filtered_labels = subject.labels[mask].reset_index(drop=True)\n",
    "    filtered_data = data_df[mask].reset_index(drop=True)\n",
    "\n",
    "    filtered_data = filtered_data.copy()\n",
    "\n",
    "    filtered_data['ECG'] = butter_filter(filtered_data['ECG'], cutoff=(0.5, 40), fs=fs_dict['ECG'], order=4, filter_type='band')\n",
    "    filtered_data['PPG'] = butter_filter(filtered_data['PPG'], cutoff=(0.5, 8), fs=fs_dict['PPG'], order=2, filter_type='band')\n",
    "    filtered_data['EDA'] = butter_filter(filtered_data['EDA'], cutoff=1, fs=fs_dict['EDA'], order=2, filter_type='low')\n",
    "    filtered_data['RESP'] = butter_filter(filtered_data['RESP'], cutoff=0.5, fs=fs_dict['RESP'], order=4, filter_type='low')\n",
    "\n",
    "    print(\"Data lista para procesar: \" + str(len(filtered_labels)))\n",
    "\n",
    "    # Calcular características\n",
    "    samples = get_samples(filtered_data, filtered_labels, fs_dict=fs_dict, stride_seconds=STRIDE_IN_SECONDS)\n",
    "\n",
    "    print(\"Características calculadas\")\n",
    "\n",
    "    if not isinstance(samples, pd.DataFrame):\n",
    "        all_samples = pd.concat(samples, ignore_index=True)\n",
    "    else:\n",
    "        all_samples = samples.copy()\n",
    "\n",
    "    all_samples['label'] = all_samples['label'].astype(int)\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "\n",
    "    # Guardar como CSV\n",
    "    all_samples.to_csv(f'{SAVE_PATH}/{subject_id}_features.csv', index=False)\n",
    "\n",
    "    # **Liberar memoria**\n",
    "    del subject, all_samples, samples, data_df, filtered_data, filtered_labels\n",
    "    gc.collect()  # Forzar recolección de basura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{SAVE_PATH}/{s}_features.csv')\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    print(df.head(10))\n",
    "    print(df.columns)\n",
    "\n",
    "    df[['1', '2', '3']] = df[['1', '2', '3']].fillna(0).astype(int)\n",
    "    df[['1', '2', '3']] = df[['1', '2', '3']].astype(int)\n",
    "    df['label'] = df[['1', '2', '3']].idxmax(axis=1).astype(int)\n",
    "    df.drop(['1', '2', '3'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{SAVE_PATH}/features.csv')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "\n",
    "    print(\"Índices en counts:\", counts.index.tolist())\n",
    "    print(\"Claves en int_to_label:\", int_to_label.keys())\n",
    "\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 4...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2407523\n",
      "1    1503835\n",
      "2    1205938\n",
      "3     954719\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3664492\n",
      "El número de ventanas esperadas es: 4846\n",
      "Progreso: 99.0508% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 6...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2389764\n",
      "1    1501181\n",
      "2    1174232\n",
      "3     917460\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3592873\n",
      "El número de ventanas esperadas es: 4751\n",
      "Progreso: 99.9790% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 7...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2398992\n",
      "1    1498312\n",
      "2    1190040\n",
      "3     918503\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3606855\n",
      "El número de ventanas esperadas es: 4770\n",
      "Progreso: 99.5807% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 8...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2415062\n",
      "1    1504388\n",
      "2    1207053\n",
      "3     944748\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3656189\n",
      "El número de ventanas esperadas es: 4835\n",
      "Progreso: 99.2761% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 9...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2409582\n",
      "1    1501991\n",
      "2    1026844\n",
      "3     938304\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3467139\n",
      "El número de ventanas esperadas es: 4583\n",
      "Progreso: 99.2799% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 10...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2404864\n",
      "1    1503313\n",
      "2    1188405\n",
      "3     903672\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3595390\n",
      "El número de ventanas esperadas es: 4754\n",
      "Progreso: 99.9159% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 11...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2525366\n",
      "1    1514550\n",
      "2    1245203\n",
      "3     971496\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3731249\n",
      "El número de ventanas esperadas es: 4935\n",
      "Progreso: 99.2908% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 13...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2396554\n",
      "1    1504210\n",
      "2    1202967\n",
      "3     940608\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3647785\n",
      "El número de ventanas esperadas es: 4824\n",
      "Progreso: 99.5025% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 15...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2455286\n",
      "1    1503014\n",
      "2    1400579\n",
      "3     987012\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3890605\n",
      "El número de ventanas esperadas es: 5148\n",
      "Progreso: 99.0676% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 19...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2452873\n",
      "1    1557803\n",
      "2    1263655\n",
      "3     998425\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3819883\n",
      "El número de ventanas esperadas es: 5054\n",
      "Progreso: 99.9209% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n",
      "Processing data for 21...\n",
      "Columnas disponibles: Index(['RESP', 'PPG', 'ECG', 'EDA'], dtype='object')\n",
      "Etiquetas: label\n",
      "0    2436666\n",
      "1    1525016\n",
      "2    1206739\n",
      "3     925992\n",
      "Name: count, dtype: int64\n",
      "Data lista para procesar: 3657747\n",
      "El número de ventanas esperadas es: 4837\n",
      "Progreso: 99.2351% completado\n",
      " Procesamiento de ventanas completado.\n",
      "Características calculadas\n"
     ]
    }
   ],
   "source": [
    "#subject_ids = [4, 6, 7, 8, 9, 10, 11, 13, 15, 19, 21]\n",
    "subject_ids = [4, 6, 7, 8, 9, 10, 11, 13, 15, 19, 21]\n",
    "\n",
    "for patient in subject_ids:\n",
    "    print(f'Processing data for {patient}...')\n",
    "    make_patient_data(patient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     HR_mean    HR_std  NN50      pNN50        TINN     rmsHRV          LF  \\\n",
      "0  64.799331  3.230688     9  29.032258  914.000000  39.770592  246.312945   \n",
      "1  65.235118  5.630104    10  31.250000  908.090909  60.664067  275.814200   \n",
      "2  64.987815  5.129443    10  31.250000  897.333333  57.220287  317.871631   \n",
      "3  64.328073  3.313657     9  28.125000  914.000000  39.608976  354.793217   \n",
      "4  64.290899  3.361635     8  25.806452  914.000000  38.530940  371.603869   \n",
      "5  64.089312  3.182478     7  22.580645  914.000000  38.275754  402.452944   \n",
      "6  63.921919  3.201196     7  22.580645  914.000000  38.783158  429.329815   \n",
      "7  64.306528  5.160580     7  21.875000  963.625000  43.966630  440.258436   \n",
      "8  64.277569  6.968145     6  19.354839  963.000000  63.117879  445.182266   \n",
      "9  63.528930  2.906676     5  16.129032  914.000000  35.052342  447.987485   \n",
      "\n",
      "           HF     LF_HF        sum_f  ...  Resp_E_std  Resp_IE_ratio  \\\n",
      "0  232.861720  1.057765  1416.924553  ...    0.776028       0.980761   \n",
      "1  213.249969  1.293384  1511.358023  ...    0.776028       0.995013   \n",
      "2  197.505072  1.609435  1560.140503  ...    0.776028       0.997609   \n",
      "3  177.392312  2.000048  1471.711506  ...    0.742725       1.008710   \n",
      "4  170.580140  2.178471  1459.917342  ...    0.782759       1.010328   \n",
      "5  154.086964  2.611856  1393.825145  ...    0.782759       0.992377   \n",
      "6  144.545258  2.970210  1344.026781  ...    0.749558       1.003479   \n",
      "7  150.108636  2.932932  1409.138243  ...    0.749558       1.003479   \n",
      "8  140.928541  3.158922  1364.918489  ...    0.785366       0.991256   \n",
      "9  138.298289  3.239284  1103.172297  ...    0.785366       0.991256   \n",
      "\n",
      "   Resp_range  Resp_insp_vol  Resp_rate  Resp_duration     1      2      3  \\\n",
      "0   10.256195       2.485396   0.333333           30.0  True  False  False   \n",
      "1   10.256195       2.671866   0.366667           30.0  True  False  False   \n",
      "2   10.256195       2.606436   0.333333           30.0  True  False  False   \n",
      "3   10.256195       2.300906   0.333333           30.0  True  False  False   \n",
      "4   10.256195       2.399265   0.333333           30.0  True  False  False   \n",
      "5   10.256195       2.566816   0.366667           30.0  True  False  False   \n",
      "6   10.256195       2.155473   0.333333           30.0  True  False  False   \n",
      "7   10.256195       2.155473   0.333333           30.0  True  False  False   \n",
      "8   10.256195       2.470446   0.366667           30.0  True  False  False   \n",
      "9   10.256195       2.470446   0.366667           30.0  True  False  False   \n",
      "\n",
      "   subject  \n",
      "0        4  \n",
      "1        4  \n",
      "2        4  \n",
      "3        4  \n",
      "4        4  \n",
      "5        4  \n",
      "6        4  \n",
      "7        4  \n",
      "8        4  \n",
      "9        4  \n",
      "\n",
      "[10 rows x 56 columns]\n",
      "Index(['HR_mean', 'HR_std', 'NN50', 'pNN50', 'TINN', 'rmsHRV', 'LF', 'HF',\n",
      "       'LF_HF', 'sum_f', 'rel_f', 'LF_norm', 'HF_norm', 'PPG_HR_mean',\n",
      "       'PPG_HR_std', 'PPG_RMSSD', 'PPG_SDNN', 'PPG_LF', 'PPG_HF', 'PPG_LF_HF',\n",
      "       'PPG_num_beats', 'PPG_RiseTime', 'PPG_DecayTime', 'PPG_PAV_mean',\n",
      "       'PPG_PAV_std', 'PPG_RI', 'EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max',\n",
      "       'EDA_range', 'EDA_slope', 'scl_mean', 'scl_std', 'scr_mean', 'scr_std',\n",
      "       'corr_scl_t', 'scr_count', 'scr_amp', 'scr_sum', 'scr_area',\n",
      "       'Resp_mean', 'Resp_std', 'Resp_I_mean', 'Resp_I_std', 'Resp_E_mean',\n",
      "       'Resp_E_std', 'Resp_IE_ratio', 'Resp_range', 'Resp_insp_vol',\n",
      "       'Resp_rate', 'Resp_duration', '1', '2', '3', 'subject'],\n",
      "      dtype='object')\n",
      "Índices en counts: [1, 2, 3]\n",
      "Claves en int_to_label: dict_keys([1, 2, 3])\n",
      "Number of samples per class:\n",
      "baseline: 21943\n",
      "stress: 17748\n",
      "cognitive load: 13646\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "subject_ids = [4, 6, 7, 8, 9, 10, 11, 13, 15, 19, 21]\n",
    "\n",
    "combine_files(subject_ids)\n",
    "print('Processing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holi\n"
     ]
    }
   ],
   "source": [
    "print('holi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
