{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import LeaveOneGroupOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNAL_CATEGORIES = {\n",
    "    'ECG': ['HR_std', 'NN50', 'pNN50', 'TINN', 'rmsHRV', 'LF', 'HF', 'LF_HF', 'sum_f', 'rel_f', 'LF_norm', 'HF_norm'],\n",
    "    'PPG': ['PPG_HR_mean', 'PPG_HR_std', 'PPG_RMSSD', 'PPG_SDNN', 'PPG_LF', 'PPG_HF', 'PPG_LF_HF', 'PPG_num_beats', \n",
    "            'PPG_RiseTime', 'PPG_DecayTime', 'PPG_PAV_mean', 'PPG_PAV_std', 'PPG_RI'],\n",
    "    'EDA': ['EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max', 'EDA_range', 'EDA_slope', 'scl_mean', 'scl_std', \n",
    "            'scr_mean', 'scr_std', 'corr_scl_t', 'scr_count', 'scr_amp', 'scr_sum', 'scr_area'],\n",
    "    'RESP': ['Resp_mean', 'Resp_std', 'Resp_I_mean', 'Resp_I_std', 'Resp_E_mean', 'Resp_E_std', 'Resp_IE_ratio', \n",
    "             'Resp_range', 'Resp_insp_vol', 'Resp_rate', 'Resp_duration'],\n",
    "}\n",
    "random = 13577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, stage):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    metrics_table = pd.DataFrame({\n",
    "        \"Métrica\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
    "        \"Valor\": [accuracy, precision, recall, f1],\n",
    "        \"Etapa\": stage\n",
    "    })\n",
    "    \n",
    "    print(f\"{stage} Metrics\")\n",
    "    print(metrics_table)\n",
    "    return metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"{title} - Confusion Matrix\")\n",
    "    print(conf_mat)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_mat, annot=True, cmap=\"Greens\", xticklabels=set(y_true), yticklabels=set(y_true), fmt='g')\n",
    "    plt.xlabel(\"Etiquetas predichas\")\n",
    "    plt.ylabel(\"Etiquetas verdaderas\")\n",
    "    plt.title(f'Matriz de confusión - {title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y, model, param_grid, model_name, signal_name):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random, stratify=y)\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=10, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f'[{signal_name} - {model_name}] Best Params: {grid_search.best_params_}')\n",
    "    print(f'[{signal_name} - {model_name}] Accuracy: {acc:.4f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_loso(X, y, subjects, model, param_grid, model_name, signal_name):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X, y, groups=subjects):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Entrenar modelo con búsqueda de hiperparámetros\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=10, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluar el mejor modelo\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        print(f'[{signal_name} - {model_name}] Best Params: {grid_search.best_params_}')\n",
    "        print(f'[{signal_name} - {model_name}] Accuracy: {acc:.4f}')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    final_acc = np.mean(accuracies)\n",
    "    print(f'[{signal_name} - {model_name}] Final LOSO Accuracy: {final_acc:.4f}')\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": (RandomForestClassifier(class_weight='balanced'), {\"n_estimators\": [50, 100, 200], \"max_depth\": [10, 20, None]}),\n",
    "    \"DecisionTree\": (DecisionTreeClassifier(class_weight='balanced'), {\"max_depth\": [5, 10, 20, None]}),\n",
    "    \"KNN\": (KNeighborsClassifier(), {\"n_neighbors\": [3, 5, 7], \"weights\": [\"uniform\", \"distance\"]})\n",
    "}\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\IA Lab\\gusgus\\WESAD_tests\\features_30_075\\features.csv'\n",
    "\n",
    "df = load_data(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training by signal:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest model for ECG...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[ECG - RandomForest] Best Params: {'max_depth': None, 'n_estimators': 200}\n",
      "[ECG - RandomForest] Accuracy: 0.9098\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.95      0.92      4389\n",
      "           2       0.93      0.90      0.91      3550\n",
      "           3       0.93      0.85      0.89      2729\n",
      "\n",
      "    accuracy                           0.91     10668\n",
      "   macro avg       0.91      0.90      0.91     10668\n",
      "weighted avg       0.91      0.91      0.91     10668\n",
      "\n",
      "Training DecisionTree model for ECG...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[ECG - DecisionTree] Best Params: {'max_depth': None}\n",
      "[ECG - DecisionTree] Accuracy: 0.7988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.82      0.82      4389\n",
      "           2       0.80      0.80      0.80      3550\n",
      "           3       0.76      0.76      0.76      2729\n",
      "\n",
      "    accuracy                           0.80     10668\n",
      "   macro avg       0.79      0.79      0.79     10668\n",
      "weighted avg       0.80      0.80      0.80     10668\n",
      "\n",
      "Training KNN model for ECG...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training by signal:  25%|██▌       | 1/4 [03:33<10:40, 213.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ECG - KNN] Best Params: {'n_neighbors': 7, 'weights': 'distance'}\n",
      "[ECG - KNN] Accuracy: 0.6316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.72      0.69      4389\n",
      "           2       0.63      0.61      0.62      3550\n",
      "           3       0.58      0.52      0.55      2729\n",
      "\n",
      "    accuracy                           0.63     10668\n",
      "   macro avg       0.62      0.62      0.62     10668\n",
      "weighted avg       0.63      0.63      0.63     10668\n",
      "\n",
      "Training RandomForest model for PPG...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training by signal:  25%|██▌       | 1/4 [03:33<10:41, 213.69s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 45 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n45 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n    X = check_array(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n    _assert_all_finite(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m model_name, (model, param_grid) \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m             acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m             results[(signal, model_name)] \u001b[38;5;241m=\u001b[39m acc\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Entrenamiento con todas las señales combinadas\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X, y, model, param_grid, model_name, signal_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m      5\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 45 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n45 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n    X = check_array(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n    _assert_all_finite(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"c:\\Users\\IA Lab\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento por cada tipo de señal\n",
    "for signal, features in tqdm(SIGNAL_CATEGORIES.items(), desc=\"Training by signal\"):\n",
    "    if all(f in df.columns for f in features):  # Verificar que todas las columnas existen\n",
    "        X = df[features]\n",
    "        y = df['label']\n",
    "        subjects = df['subject']  # Se agrega el identificador de sujetos para LOSO\n",
    "        \n",
    "        for model_name, (model, param_grid) in models.items():\n",
    "            print(f'Training {model_name} model for {signal}...')\n",
    "            acc = train_and_evaluate(X, y, model, param_grid, model_name, signal)\n",
    "            results[(signal, model_name)] = acc\n",
    "\n",
    "# Entrenamiento con todas las señales combinadas\n",
    "X_all = df.drop(columns=['label', 'subject'])  # También se excluye 'subject'\n",
    "y_all = df['label']\n",
    "subjects_all = df['subject']\n",
    "\n",
    "for model_name, (model, param_grid) in tqdm(models.items(), desc=\"Training with ALL signals\"):\n",
    "    print(f'Training {model_name} model for ALL signals using LOSO...')\n",
    "    acc = train_and_evaluate_loso(X_all, y_all, subjects_all, model, param_grid, model_name, \"ALL\")\n",
    "    results[(\"ALL\", model_name)] = acc\n",
    "\n",
    "# Mostrar resultados finales\n",
    "print(\"\\nFinal Comparison:\")\n",
    "for (signal, model_name), acc in results.items():\n",
    "    print(f'{signal} - {model_name}: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RANDOM FOREST*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gridrRD = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "randomfor_model = RandomForestClassifier(random_state=random)\n",
    "\n",
    "print(\"Sin balanceo\")\n",
    "train_and_evaluate(randomfor_model, param_gridrRD, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADA BOOST*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gridADA = {\n",
    "    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 58],\n",
    "    'estimator__max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30]\n",
    "}\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(random_state=random)\n",
    "adaboost_model = AdaBoostClassifier(estimator=base_estimator, random_state=random)\n",
    "\n",
    "train_and_evaluate(adaboost_model, param_gridADA, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*KNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gridKNN = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 8, 10, 12, 15, 18, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree']\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "train_and_evaluate(knn_model, param_gridKNN, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DECISION TREE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gridDT = {\n",
    "    'criterion': ['gini', 'entropy'],  # Función para medir la calidad de una división\n",
    "    'max_depth': [None, 3, 5, 10, 20],  # Profundidad máxima del árbol\n",
    "    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4, 8],  # Muestras mínimas en una hoja\n",
    "    'max_features': ['sqrt', 'log2', None]  # Número de características a considerar en cada división\n",
    "}\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=random)\n",
    "\n",
    "train_and_evaluate(decision_tree_model, param_gridDT, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LDA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gridLDA = {\n",
    "    'solver': ['lsqr', 'eigen'],  # Excluye 'svd'\n",
    "    'shrinkage': ['auto', 0.1, 0.5, 1.0]  # Compatible con 'lsqr' y 'eigen'\n",
    "}\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "train_and_evaluate(lda_model, param_gridLDA, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
